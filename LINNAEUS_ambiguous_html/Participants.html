
<!doctype html><html lang="en"> <head><meta charset="utf-8"> 
<title>LINNAEUS inconsistencies</title> 
<style>  
.yellow { background-color:rgba(50, 180, 180, 0.5); } 
.red { background-color:rgba(214, 75, 79, 0.5); } 
.blue { background-color:rgba(75, 75, 214, 0.5); } 
table { border-collapse: collapse; } 
th, td { border: 1px solid black; padding: 4px; } 
th {  cursor: pointer; } 
th:hover { background: yellow; }
</style></head><body>
<h2>Participants</h2><span class="red">Not tagged name</span> - <span class="blue">Tagged name</span> - <span class="yellow">Other name</span><br><hr><h3>pmcA1526545</h3>B lymphocyte stimulator (BLyS) isoforms in systemic lupus erythematosus: disease activity correlates better with blood leukocyte BLyS mRNA levels than with plasma BLyS protein levels
Abstract
Considerable evidence points to a role for B lymphocyte stimulator (BLyS) overproduction in <span class="yellow">murine</span> and <span class="yellow">human</span> systemic lupus erythematosus (SLE). Nevertheless, the correlation between circulating levels of BLyS protein and disease activity in <span class="yellow">human</span> SLE is modest at best. This may be due to an inadequacy of the former to reflect endogenous BLyS overproduction faithfully, in that steady-state protein levels are affected not just by production rates but also by rates of peripheral utilization and excretion. Increased levels of BLyS mRNA may better reflect increased in vivo BLyS production, and therefore they may correlate better with biologic and clinical sequelae of BLyS overexpression than do circulating levels of BLyS protein. Accordingly, we assessed peripheral blood leukocyte levels of BLyS mRNA isoforms (full-length BLyS and ΔBLyS) and plasma BLyS protein levels in <span class="yellow">patients</span> with SLE, and correlated these levels with laboratory and clinical features. BLyS protein, full-length BLyS mRNA, and ΔBLyS mRNA levels were greater in SLE <span class="yellow">patients</span> (n = 60) than in rheumatoid arthritis <span class="yellow">patients</span> (n = 60) or normal control individuals (n = 30). Although full-length BLyS and ΔBLyS mRNA levels correlated significantly with BLyS protein levels in the SLE cohort, BLyS mRNA levels were more closely associated with serum immunoglobulin levels and SLE Disease Activity Index scores than were BLyS protein levels. Moreover, changes in SLE Disease Activity Index scores were more closely associated with changes in BLyS mRNA levels than with changes in BLyS protein levels among the 37 SLE <span class="yellow">patients</span> from whom repeat blood samples were obtained. Thus, full-length BLyS and ΔBLyS mRNA levels are elevated in SLE and are more closely associated with disease activity than are BLyS protein levels. BLyS mRNA levels may be a helpful biomarker in the clinical monitoring of SLE <span class="yellow">patients</span>.<br><br>Introduction
B lymphocyte stimulator (BLyS; a trademark of <span class="yellow">Human</span> Genome Sciences, Inc., Rockville, MD, USA) is a 285-amino-acid member of the tumor necrosis factor ligand superfamily [1-3]. A causal relation between constitutive overproduction of BLyS and development of systemic lupus erythematosus (SLE)-like illness has incontrovertibly been established in <span class="yellow">mice</span>. BLyS-transgenic <span class="yellow">mice</span> often develop SLE-like features as they age [3-5], and SLE-prone (NZB × NZW)F1 (BWF1) and MRL-lpr/lpr <span class="yellow">mice</span> respond clinically to treatment with BLyS antagonists (decreased disease progression and improved survival) [3,6].
Considerable inferential evidence points to a role for BLyS overproduction in <span class="yellow">human</span> SLE as well. Cross-sectional studies have demonstrated elevated circulating levels of BLyS in 20–30% of <span class="yellow">human</span> SLE <span class="yellow">patients</span> tested at a single point in time [7,8]. Moreover, a 12-month longitudinal study documented persistently elevated serum BLyS levels in about 25% of SLE <span class="yellow">patients</span> and intermittently elevated serum BLyS levels in an additional 25% of <span class="yellow">patients</span> [9]. Remarkably, circulating BLyS levels did not correlate with disease activity (measured using the SLE Disease Activity Index [SLEDAI]) in these cross-sectional or longitudinal studies [7-9]. Although a statistically significant correlation between circulating BLyS levels and SLEDAI has been appreciated in a more recent 24-month longitudinal study of 245 SLE <span class="yellow">patients</span> (with >1,700 plasma samples analyzed) [10], the correlation remains weak.
The limited correlation between circulating BLyS protein levels and disease activity in these studies may have exposed an inadequacy of the former to reflect faithfully endogenous BLyS overproduction. In addition to the rate of BLyS protein production, several other factors (for example, utilization and excretion) can affect circulating BLyS protein levels. Although there are no practicable means of directly measuring in vivo BLyS production per se in <span class="yellow">humans</span>, the level of BLyS mRNA may serve as a better surrogate marker of in vivo BLyS production than does the level of BLyS protein. Candidate BLyS mRNA isoforms include the full-length BLyS mRNA isoform, which encodes the full-length protein, and the alternatively spliced ΔBLyS mRNA isoform, which encodes a protein with a small peptide deletion [11]. (ΔBLyS does not bind to cells expressing BLyS receptors, and therefore it has no agonistic activity. Moreover, ΔBLyS can form heterotrimers with full-length BLyS, thereby actually functioning as a dominant-negative antagonist of BLyS activity.)
In this report we demonstrate that peripheral blood leukocytes from SLE <span class="yellow">patients</span> express elevated mRNA levels of both full-length BLyS and ΔBLyS relative to those levels expressed by <span class="yellow">patients</span> with rheumatoid arthritis (RA) or by normal control individuals. In the SLE <span class="yellow">patients</span>, both full-length BLyS and ΔBLyS mRNA levels are more closely associated with disease activity (SLEDAI) than are BLyS protein levels. Accordingly, BLyS mRNA levels may be a helpful biomarker in the clinical monitoring of SLE <span class="yellow">patients</span>.<br><br>Materials and methods
General details
This study was approved by the institutional review boards of the University of Southern California and the Scripps Research Institute. All <span class="yellow">participants</span> gave their written informed consent before participation in this study.<br><br><span class="blue">Participants</span>
<span class="yellow">Patients</span> receiving outpatient medical care at the rheumatology clinics of the Los Angeles County + University of Southern California Medical Center were recruited into the study. Diagnoses of SLE (n = 60) or RA (n = 60) were based on established clinical criteria [12]. Healthy control individuals (n = 30) were recruited from Los Angeles County + University of Southern California Medical Center and University of Southern California Keck School of Medicine personnel. No exclusions were made on any basis other than an inability to give informed consent. Each <span class="yellow">patient</span>'s sex, race, age, and medications at the time of the phlebotomy were recorded (Table 1).
Based solely on the <span class="yellow">patient</span>'s willingness to donate a second blood sample, repeat blood samples were collected from 37 of the SLE <span class="yellow">patients</span> 147–511 days (median 371 days) after collection of the first samples. These <span class="yellow">patients</span> were not selected on the basis of any demographic, clinical, or laboratory feature.
Clinical disease activity for the SLE <span class="yellow">patients</span> was assessed using the SLEDAI [13] and using a modified SLEDAI that excludes the contribution of anti-double-stranded DNA (anti-dsDNA) antibodies from the total score. Each <span class="yellow">patient</span>'s medical chart was reviewed for results of standard clinical laboratory tests within the previous or subsequent 1-month period.<br><br>Plasma BLyS determination
Whole venous blood was centrifuged to yield plasma and a buffy coat. The plasma was harvested, stored at -70°C, and assayed for BLyS levels by ELISA [8,14] using Fab fragments of the capture antibody rather than the whole antibody to reduce assay interference by rheumatoid factor. The lower limit of detection in this assay is 0.3 ng/ml. For statistical purposes, plasma samples with BLyS concentrations below the lower limit of detection were assigned a value of 0.25 ng/ml.<br><br>Blood BLyS mRNA determination
The buffy coat from centrifuged whole blood was harvested, added to RNAlater™ (Ambion, Austin, TX, USA) at a 1:4 vol/vol ratio for RNA stabilization, stored at -70°C, and assayed for full-length BLyS and ΔBLyS mRNA levels by real-time PCR. Total RNA was purified from buffy coat samples using RNAeasy miniprep kits (Qiagen, Valencia, CA, USA), and contaminating genomic DNA was removed by DNAse-I digestion. One-tenth volume of total RNA was used as template in the first-strand cDNA reaction using oligo-dT and the Superscript III first-strand synthesis system (Invitrogen, Carlsbad, CA, USA). Duplicate samples of cDNA were amplified with primers against β-actin, full-length BLyS, or ΔBLyS: β-actin sense 5'-CGAGAAGATGACCCAGATCATGT-3'; β-actin anti-sense 5'-GGCATACCCCTCGTAGATGG-3'; full-length BLyS sense 5'-GCAGACAGTGAAACACCAACTATAC-3'; ΔBLyS sense 5'-CAGAAGAAACAGGATCTTACACAT-3'; and full-length BLyS/ΔBLyS anti-sense 5'-TGCCAGCTGAATAGCAGGAATTAT-3'.
A 165 bp amplicon for β-actin was PCR-amplified using the 7900 HT ABI Prism machine (Qiagen) with annealing at 65°C. A 296 bp amplicon for full-length BLyS was PCR-amplified, with annealing at 64°C. A 270 bp amplicon for ΔBLyS was PCR-amplified with annealing at 61°C. The annealing conditions for full-length BLyS and ΔBLyS were determined so that each primer set remained specific to the respective BLyS isoform and yielded a PCR efficiency similar to those of cloned cDNA standards. Melting curve analysis revealed a single peak for each gene amplified. The threshold cycle (Ct) values for each reaction were determined using Sequence Detection System software (Applied Biosystems, Foster City, CA, USA). Results are presented as ratios of full-length BLyS or ΔBLyS mRNA to β-actin mRNA, which were calculated using the following formulae:
2 exp(Ctβ-actin - Ctfull-length BLyS)
2 exp(Ctβ-actin - CtΔBLyS)<br><br>Determination of anti-BLyS autoantibodies
BLyS was bound to microtiter plates by first coating the plates with streptavidin and then adding biotinylated recombinant BLyS. Using these plates as the capture reagent, plasma samples were incubated, and <span class="yellow">horseradish</span> peroxidase-conjugated anti-<span class="yellow">human</span> IgA/IgM/IgG (Southern Biotechnology Associates, Birmingham, AL, USA; 1:20,000 final dilution) or <span class="yellow">horseradish</span> peroxidase-conjugated anti-<span class="yellow">human</span> IgG (Southern Biotechnology; 1:10,000 final dilution) were used as the detector reagents.<br><br>Statistical analysis
All analyses were performed using SigmaStat software (SPSS, Chicago, IL, USA). Results that did not follow a normal distribution were log-transformed to achieve normality. Parametric testing between two matched or unmatched groups was performed using the paired or unpaired t test, respectively. Parametric testing among three or more groups was performed using one-way analysis of variance. When log-transformation failed to generate normally distributed data or the equal variance test was not satisfied, nonparametric testing was performed using the Mann–Whitney rank sum test between two groups and by Kruskal–Wallis one-way analysis of variance on ranks among three or more groups. Correlations were determined using Pearson product moment correlation for interval data and using Spearman rank order correlation for ordinal data or for interval data that did not follow a normal distribution. Nominal data were analyzed using χ2 analysis-of-contingency tables.<br><br>
Results
Elevated plasma BLyS levels and blood levels of full-length BLyS and ΔBLyS mRNA isoforms in systemic lupus erythematosus <span class="yellow">patients</span>
Previous reports of elevated circulating BLyS levels in SLE <span class="yellow">patients</span> were based on a BLyS ELISA that utilized a whole (unfragmented) capture anti-BLyS monoclonal antibody [7-9]. Since the publication of these reports, it has been recognized that the presence of rheumatoid factor can potentially interfere with the assay and lead to spurious overestimation of the true circulating BLyS levels (<span class="yellow">Human</span> Genome Sciences, Inc.; unpublished observations). To mitigate potential interference from rheumatoid factor, the BLyS ELISA has been modified and the capture anti-BLyS monoclonal antibody is now utilized as a Fab fragment. Despite the changes in the ELISA format, our findings are entirely consistent with those of the previous reports. Plasma BLyS levels were significantly greater in the SLE group than in either RA or normal control group (P < 0.001; Figure 1a). Arbitrary assignment of the 95th percentile value among the normal control individuals as the upper limit of 'normal' revealed that two of the 30 normal control individuals, 15 of the 60 RA <span class="yellow">patients</span>, and 29 of the 60 SLE <span class="yellow">patients</span> harbored elevated plasma BLyS levels (P < 0.001).
Overexpression of BLyS in SLE <span class="yellow">patients</span> was also established by measuring BLyS mRNA levels normalized to β-actin mRNA levels in peripheral blood leukocytes (buffy coats). The geometric mean full-length BLyS mRNA and ΔBLyS mRNA levels among the SLE <span class="yellow">patients</span> were each significantly greater than those among the RA <span class="yellow">patients</span> and normal control individuals, respectively (P < 0.001 for each; Figure 1b,c). Arbitrary assignment of the 95th percentile values for full-length BLyS and ΔBLyS mRNA levels among the normal control individuals as the upper limits of 'normal' revealed that two of the 30 normal control individuals, four of the 60 RA <span class="yellow">patients</span>, and 20 of the 60 SLE <span class="yellow">patients</span> had elevated full-length BLyS mRNA levels (P < 0.001), and that two of the 30 normal control individuals, three of the 60 RA <span class="yellow">patients</span>, and 19 of the 60 SLE <span class="yellow">patients</span> had elevated ΔBLyS mRNA levels (P < 0.001). Levels of full-length BLyS and ΔBLyS mRNA strongly correlated with each other (r = 0.703; P < 0.001) in the SLE cohort, and plasma BLyS levels also correlated significantly with levels of each BLyS isoform (r = 0.429, P < 0.001; and r = 0.290, P = 0.024, respectively). Among these SLE <span class="yellow">patients</span>, none of the measured BLyS parameters correlated with <span class="yellow">patient</span> age, sex, race, or daily dose of corticosteroids (data not shown). Because the racial composition of the normal cohort was not as predominantly Hispanic as were those of the RA and SLE cohorts, we assessed the BLyS parameters in the respective Hispanic subpopulations. As for the entire populations, values for SLE were significantly greater than those for either RA or normal controls (P ≤ 0.004; data not shown).<br><br>Correlations between BLyS parameters and plasma immunoglobulin levels
BLyS is a potent B cell survival factor [15-21], and administration of exogenous BLyS to <span class="yellow">mice</span> leads to B cell expansion and hypergammaglobulinemia [1]. Previous studies with numbers of SLE <span class="yellow">patients</span> greater than were included in the present study documented a modest but significant correlation between serum levels of BLyS and IgG [8,10]. In our SLE cohort of limited size, plasma BLyS levels failed to show significant correlations with plasma levels of total immunoglobulin, IgG, or IgA. In contrast, full-length BLyS and ΔBLyS mRNA levels correlated significantly with each (Figure 2). (None of the BLyS parameters correlated with plasma IgM levels.) The absence of significant correlation between plasma BLyS levels and the immunoglobulin parameters also persisted when just the 53 <span class="yellow">patients</span> with detectable plasma BLyS levels were considered (r = -0.133, P = 0.346 for total immunoglobulin; r = -0.048, P = 0.734 for IgG; and r = 0.033, P = 0.817 for IgA).<br><br>Correlations between BLyS parameters and disease activity
Previous studies either have failed to demonstrate a significant correlation between disease activity and circulating BLyS levels [7-9] or have detected only a weak correlation between the two [10]. Consonant with those studies, we identified no significant correlation between plasma BLyS levels and SLEDAI in our cohort of 60 SLE <span class="yellow">patients</span> (Figure 3a). The failure to demonstrate a significant correlation cannot be attributed to a skewing of the results by the <span class="yellow">patients</span> in whom plasma BLyS levels were below the limit of detection, because no significant correlation was detected among the 53 SLE <span class="yellow">patients</span> in whom plasma BLyS levels were in the detectable range (r = 0.185, P = 0.183). In contrast, a significant correlation between SLEDAI and full-length BLyS mRNA levels was readily discernible (Figure 3b). A trend toward a correlation between SLEDAI and ΔBLyS mRNA levels was also observed, although it did not achieve statistical significance (Figure 3c).
A component of the SLEDAI is the presence of circulating anti-dsDNA antibodies. Because circulating BLyS levels may affect the presence and/or titers of circulating anti-dsDNA antibodies [7-10], we assessed correlations between the individual BLyS parameters and a modified SLEDAI that excludes any consideration of anti-dsDNA antibodies. As with the unmodified SLEDAI, the modified SLEDAI did not correlate with plasma BLyS levels (Figure 3d) either among the SLE cohort overall or among the 53 <span class="yellow">patients</span> in whom plasma BLyS levels were in the detectable range (r = 0.160, P = 0.252), but it significantly correlated with full-length BLyS mRNA levels (Figure 3e) and exhibited a trend toward correlation with ΔBLyS mRNA levels (Figure 3f). Thus, the stronger correlations between BLyS mRNA levels and disease activity cannot solely be explained by any effects that BLyS may have on anti-dsDNA antibodies per se.
Moreover, among the 37 SLE <span class="yellow">patients</span> who were evaluated on two separate occasions, trends toward correlation were appreciated between changes in the unmodified or modified SLEDAI and changes in full-length BLyS or ΔBLyS mRNA levels but not changes in plasma BLyS levels (Figure 4). These results cannot be ascribed to changes in medications taken by the <span class="yellow">patients</span>, because changes in neither disease activity nor in any of the BLyS parameters correlated with changes in the doses of corticosteroids or cytotoxics taken by the <span class="yellow">patients</span> (data not shown). The failure to demonstrate a meaningful association between changes in SLEDAI score and changes in plasma BLyS protein levels cannot be attributed to a skewing of the results by the <span class="yellow">patients</span> in whom plasma BLyS levels were below the limit of detection, because the absence of association between the two persisted among the 27 SLE <span class="yellow">patients</span> in whom plasma BLyS levels were in the detectable range in both samples (r = -0.069, P = 0.727 for plasma BLyS versus unmodified SLEDAI; r = -0.020, P = 0.919 for plasma BLyS versus modified SLEDAI).<br><br>Lack of correlation between levels of BLyS mRNA isoforms and percentages of individual leukocyte cell types
Among cells in peripheral blood, BLyS is predominantly expressed by cells of the myeloid lineage (monocytes and neutrophils) [1,14,22,23]. Accordingly, a shift in the differential leukocyte count away from lymphocytes to monocytes and/or neutrophils could substantially alter BLyS mRNA results. Because of the limited amount of blood we were permitted to obtain from the SLE <span class="yellow">patients</span> (consequent to the high prevalence of anemia among these <span class="yellow">patients</span>), we were unable to purify the individual leukocyte populations for BLyS mRNA analysis. Nevertheless, to demonstrate that the elevated BLyS mRNA levels in SLE did not simply reflect a shift in differential leukocyte count, we assessed the correlations between the individual BLyS parameters on the one hand and the percentages of blood neutrophils, monocytes, and lymphocytes on the other. No correlations were appreciated (Figure 5).<br><br>Presence of anti-BLyS autoantibodies in <span class="yellow">patients</span> with systemic lupus erythematosus
The poorer correlation between plasma BLyS protein levels and disease activity compared with that between BLyS mRNA levels and disease activity was striking. <span class="yellow">Patients</span> with SLE frequently develop autoantibodies against self-antigens, and so some of the SLE <span class="yellow">patients</span> might have harbored autoantibodies to BLyS. Such autoantibodies could have complexed with BLyS and enhanced its clearance, thereby masking BLyS overproduction. Alternatively, such autoantibodies might have sterically blocked the epitopes recognized by the detecting antibodies in the in vitro ELISA. In this case, measured BLyS levels would have been spuriously reduced, again masking BLyS overproduction.
In our cohort, IgA/IgM/IgG anti-BLyS antibodies were detected in six out of the 60 SLE <span class="yellow">patients</span>. Such autoantibodies were also detected in two out of 60 RA <span class="yellow">patients</span> and in one out of 30 normal control individuals, demonstrating that anti-BLyS autoantibodies are not restricted to SLE <span class="yellow">patients</span>. IgG anti-BLyS autoantibodies were detected in 3 SLE <span class="yellow">patients</span> but in no RA <span class="yellow">patients</span> or normal control individuals.<br><br>
Discussion
Elevated blood levels of BLyS protein and mRNA are well described features of <span class="yellow">human</span> SLE [7-9]. We confirmed these observations in our study and extended them by documenting increases not just in levels of full-length BLyS mRNA but also in levels of ΔBLyS mRNA (Figure 1). Of note, BLyS mRNA levels were elevated in SLE but not in RA, raising the possibility that BLyS overproduction in SLE is systemic whereas BLyS overproduction in RA may be more focused to the affected arthritic joints [24]. The modestly elevated plasma BLyS protein levels in RA <span class="yellow">patients</span> may reflect, at least in part, release of locally overproduced BLyS into the circulation.
The relationship between circulating BLyS protein levels and disease activity was addressed in several previous studies, but significant correlations between the two measures did not emerge [7-9]. In the largest study to date, a 2-year longitudinal study of 245 <span class="yellow">patients</span> in which more than 1,700 plasma samples were analyzed, a significant but weak correlation between the two was appreciated [10]. In the present study, a significant correlation between plasma BLyS protein levels and disease activity was again not realized (Figure 3a,d).
The weak, at best, correlation between circulating BLyS levels and disease activity is seemingly rather surprising. There is a clear-cut association in BlyS transgenic <span class="yellow">mice</span> between BLyS overexpression and development of SLE-like features [3-5], and treatment of SLE-prone <span class="yellow">mice</span> with BLyS antagonists retards the progression of disease and improves survival [3,6]. Moreover, development of precocious glomerular pathology in autoimmune-prone <span class="yellow">mice</span> correlates strongly with circulating BLyS levels [25].
The likely explanation for the weak correlation between circulating BLyS levels and disease activity in <span class="yellow">human</span> SLE is not that disease activity in SLE <span class="yellow">patients</span> is insensitive to the degree of BLyS overproduction. Rather, a more tenable explanation is that circulating BLyS levels in <span class="yellow">human</span> SLE do not always accurately reflect excessive endogenous BLyS production. We can identify at least three nonmutually exclusive mechanisms to explain a dissociation between the two.
First, SLE <span class="yellow">patients</span> frequently develop autoantibodies to a myriad of self-targets (for example, erythrocytes, lymphocytes). Indeed, we detected circulating IgA/IgM/IgG anti-BLyS autoantibodies in 10% (6/60) of the tested SLE <span class="yellow">patients</span>, and we detected circulating IgG anti-BLyS autoantibodies in 5% (3/60). These percentages may be underestimates of the true prevalence of anti-BLyS autoantibodies, because some of these autoantibodies may be saturated in vivo with circulating BLyS, rendering them incapable of binding to BLyS in the in vitro detection assay. We do not yet know whether the anti-BLyS autoantibodies are functionally neutralizing but, regardless, such autoantibodies could enhance the clearance of BLyS and/or interfere with in vitro detection of BLyS, thereby masking endogenous BLyS overproduction.
Second, increased urinary excretion of BLyS has been reported in SLE <span class="yellow">patients</span>, especially among those with clinically overt renal involvement [26]. At least four of the <span class="yellow">patients</span> we studied manifested nephrotic-range proteinuria (≥3 g/24 hours), and so urinary loss of BLyS was probably substantial in at least these <span class="yellow">patients</span>. A validated assay for urinary BLyS detection has not yet been developed so we were unable to quantify urinary BLyS levels. Once an assay for urinary BLyS levels is validated, we should be able to assess the effect of urinary BLyS excretion on circulating BLyS levels.
Third, BLyS promotes in vivo expansion of B cells [1]. Freshly isolated SLE B cells, despite intact surface expression of BLyS receptors, bind less biotinylated BLyS ex vivo than do freshly isolated normal B cells [27]. Although other interpretations are possible, the most likely explanation is that BLyS receptors on B cells in SLE <span class="yellow">patients</span> are occupied in vivo by soluble BLyS. Accordingly, it is likely that BLyS receptors expressed by the expanded B cell population do bind BLyS and remove it from the circulation, resulting in a homeostatic pathway that modulates the effects of BLyS overproduction on circulating BLyS levels. Indeed, circulating levels of BLyS rise with peripheral blood B cell depletion and fall with re-emergence of peripheral blood B cells in rituximab-treated RA or SLE <span class="yellow">patients</span> [28,29], highlighting this inverse relationship between circulating BLyS levels and B cell load. Moreover, one of the hallmarks of active disease in <span class="yellow">human</span> SLE is the increased percentages of activated B cells and plasma cells in peripheral blood [30-34], probably reflecting increased systemic numbers of activated B cells and plasma cells. Although not yet formally tested, differential BLyS receptor expression by these cells compared with expression by nonactivated B cells may result in increased peripheral BLyS utilization, further dampening the effects of BLyS overproduction on circulating protein levels.
To circumvent these confounding processes, we used BLyS mRNA levels as a surrogate marker of endogenous BLyS production. Overall, the correlations between disease activity and either full-length BLyS or ΔBLyS mRNA levels were much stronger than that between disease activity and BLyS protein levels (Figures 3 and 4). This was the case regardless of whether we used the standard SLEDAI or the modified SLEDAI as a measure of disease activity. These correlations were not spurious ones consequent to shifts in percentages of leukocyte subpopulations in peripheral blood, because BLyS mRNA levels did not correlate with percentages of blood neutrophils, monocytes, or lymphocytes (Figure 5).
A similar pattern was observed between plasma immunoglobulin levels and the BLyS parameters, with plasma levels of total immunoglobulin, IgG, and IgA correlating significantly with full-length BLyS and ΔBLyS mRNA levels but not with plasma BLyS levels (Figure 2). These significant correlations between full-length BLyS or ΔBLyS mRNA levels and plasma immunoglobulin levels again highlight the greater ability of BLyS mRNA levels, compared with plasma BLyS protein levels, to reflect ongoing BLyS overproduction.
At present, it is not known whether soluble ΔBLyS protein is present in the circulation of SLE <span class="yellow">patients</span> or of normal individuals. Although full-length BLyS protein is readily cleaved and released from cells transfected with a vector containing <span class="yellow">murine</span> full-length BLyS, ΔBLyS protein is not cleaved or released from <span class="yellow">murine</span> ΔBLyS transfectants [11]. Given the strong similarities between <span class="yellow">murine</span> and <span class="yellow">human</span> full-length BLyS and ΔBLyS, it is likely that <span class="yellow">human</span> soluble ΔBLyS protein is also not cleaved from the cell surface and released into the circulation. Moreover, soluble ΔBLyS protein is not released from cells transfected with a vector containing just the extracellular domain of <span class="yellow">human</span> ΔBLyS (which encodes the soluble protein; A.L. Gavin, unpublished observations). Whether this reflects rapid intracellular degradation of soluble ΔBLyS or some other impediment to its release remains unknown. Regardless, if the inability to release soluble ΔBLyS in vitro faithfully recapitulates in vivo biology, then the stronger associations between SLE disease activity and full-length BLyS or ΔBLyS mRNA levels compared with that between SLE disease activity and BLyS protein levels could not be attributable to interference by biologically inactive (inhibitory) ΔBLyS protein in the BLyS protein detection ELISA. Importantly, even if soluble ΔBLyS protein is present in the circulation and is detected by the BLyS protein detection ELISA, then the stronger correlations between SLE disease activity and full-length BLyS or ΔBLyS mRNA levels than that between disease activity and total BLyS (including ΔBLyS) protein levels suggest that full-length BLyS and/or ΔBLyS mRNA levels may operationally serve as useful biomarkers of disease activity in SLE. Although the complexity and labor intensiveness associated with quantitative real-time PCR may render measurement of BLyS mRNA levels impracticable for routine clinical practice, such measurement could readily be incorporated into clinical trials and yield valuable information.
Longitudinal observations in large numbers of SLE <span class="yellow">patients</span> will be necessary to establish or refute the utility of full-length BLyS and/or ΔBLyS mRNA to subserve this clinically vital function.
Although expression of the two major BLyS isoforms was highly coordinate among SLE <span class="yellow">patients</span>, there were several <span class="yellow">patients</span> in whom ΔBLyS mRNA levels were markedly greater than or less than the expected values based on full-length BLyS mRNA levels (data not shown). This raises the possibility that dysregulation of ΔBLyS may contribute to overall BLyS dysregulation in at least some SLE <span class="yellow">patients</span>. It is known that interferon-γ, interleukin-10, interferon-α, and CD154 can upregulate full-length BLyS mRNA levels [14,22,35], but it is not known what effects these or other cytokines/cell-surface structures have on ΔBLyS expression. Further investigation of the regulation of ΔBLyS and the differential expression of BLyS isoforms is certainly warranted.
Although the associations between full-length BLyS and/or ΔBLyS mRNA levels and disease activity in SLE were usually strong when the SLE cohort was analyzed in aggregate, there were several SLE <span class="yellow">patients</span> in whom BLyS mRNA levels were quite high despite little objective ongoing disease activity, and there were several SLE <span class="yellow">patients</span> in whom BLyS mRNA levels were low despite considerable ongoing disease activity. One must recognize that the bulk of the pathogenic autoimmune responses probably takes place in the spleen and lymph nodes, rather than in the peripheral blood, where myeloid lineage cells (for example, dendritic cells) produce BLyS and support B cell survival and expansion [36]. Local BLyS production in the secondary lymphoid tissues will be more important to the development and maintenance of an autoimmune response than will remote BLyS levels in the circulation. Because at least some autoreactive B cells may be more sensitive to BLyS-mediated survival signals than non-autoreactive B cells [37,38], local increases in BLyS production could preferentially promote expansion of autoreactive B cells. These cells, in turn, could activate autoreactive T cells by presenting autoantigen to them, and some of the autoreactive B cells would respond to T cell derived signals and mature into (pathogenic) autoantibody secreting plasma cells. In contrast to <span class="yellow">murine</span> studies, in which investigators can readily harvest and analyze lymphoid and myeloid lineage cells from any site (for example, spleen, bone marrow), such is not the case for <span class="yellow">human</span> studies. Peripheral blood is the only site readily accessible for <span class="yellow">human</span> studies, and it is possible that, at least in some <span class="yellow">patients</span>, BLyS mRNA levels in circulating leukocytes do not reflect local BLyS production in the secondary lymphoid tissues.
One must also recognize that disease activity in SLE is not solely driven by B cells. Systemic inflammation and SLE flares can be triggered via B cell independent means. Not all SLE <span class="yellow">patients</span> treated with a B cell depleting course of rituximab experience clinical remission [39], strongly pointing to the importance of non-B cells in disease pathogenesis/maintenance. Conversely, not all pathogenic B cells necessarily require high levels of BLyS to effect their pathogenicity. <span class="yellow">Murine</span> studies have unequivocally documented B cell subpopulations that do not depend upon BLyS for their survival [40-42]. Although <span class="yellow">mice</span> completely devoid of BLyS have reduced numbers of mature B cells and harbor reduced levels of immunoglobulin, these reductions are incomplete. Thus, it is possible that some SLE <span class="yellow">patients</span> harbor pathogenic B cells that are relatively insensitive to BLyS and could drive considerable disease activity even in the context of low endogenous BLyS production. Conversely, <span class="yellow">patients</span> with high BLyS mRNA levels may be those <span class="yellow">patients</span> whose disease is strongly driven by BLyS and may be especially helped by BLyS antagonist therapy. Future clinical trials should be able to establish whether the BLyS mRNA levels are good predictors of response to such agents.<br><br>Conclusion
Plasma total immunoglobulin, IgG, and IgA levels and disease activity (as measured by SLEDAI) in SLE <span class="yellow">patients</span> correlate more closely with peripheral blood leukocyte levels of BLyS mRNA than with plasma levels of BLyS protein. These findings suggest that BLyS mRNA levels better reflect in vivo BLyS production than do circulating BLyS protein levels, and may be a useful biomarker in the clinical monitoring of SLE <span class="yellow">patients</span>. These findings also support the premise that BLyS overexpression not only promotes development of disease but also actively contributes to the ongoing maintenance of disease in SLE <span class="yellow">patients</span>. This reinforces the rationale underlying clinical trials with BLyS antagonists in SLE.<br><br>Abbreviations
anti-dsDNA = anti-double-stranded DNA; BLyS = B lymphocyte stimulator; bp = base pairs; Ct = threshold cycle; ELISA = enzyme-linked immunosorbent assay; PCR = polymerase chain reaction; RA = rheumatoid arthritis; SLE = systemic lupus erythematosus; SLEDAI = SLE Disease Activity Index.<br><br>Competing interests
TSM and DMH were employees of <span class="yellow">Human</span> Genome Sciences (HGS) at the time the investigation was conducted. (DMH has since left the company.) WS has received research support from HGS and has served as a consultant to HGS (<$10,000). CEC, ALG, and DN declare that they have no competing interests.<br><br>Authors' contributions
CEC identified and recruited all <span class="yellow">participants</span>; collected all the blood samples and reviewed all the medical charts; and wrote the initial working draft of this manuscript. ALG developed and performed all the real-time PCR assays and assisted in the interpretation of the results and in writing the final version of the manuscript. TSM performed the plasma BLyS protein and anti-BLyS assays and assisted in the interpretation of the results and in writing the final version of the manuscript. DMH assisted in the design in the study, in the interpretation of the results, and in writing the final version of the manuscript. DN assisted in the design in the study, in the interpretation of the results, and in writing the final version of the manuscript. WS conceived the study, supervised the recruitment of <span class="yellow">participants</span>, performed the statistical analyses, assisted in the interpretation of the results, and supervised the editing of the manuscript to its final form. All authors read and approved the final manuscript version.<br><br>
<h3>pmcA2602716</h3>A Case-Control Study to Assess the Relationship between Poverty and Visual Impairment from Cataract in Kenya, the Philippines, and Bangladesh
Abstract
Background
The link between poverty and health is central to the Millennium Development Goals (MDGs). Poverty can be both a cause and consequence of poor health, but there are few epidemiological studies exploring this complex relationship. The aim of this study was to examine the association between visual impairment from cataract and poverty in adults in Kenya, Bangladesh, and the Philippines.<br><br>Methods and Findings
A population-based case–control study was conducted in three countries during 2005–2006. Cases were <span class="yellow">persons</span> aged 50 y or older and visually impaired due to cataract (visual acuity < 6/24 in the better eye). Controls were <span class="yellow">persons</span> age- and sex-matched to the case <span class="yellow">participants</span> with normal vision selected from the same cluster. Household expenditure was assessed through the collection of detailed consumption data, and asset ownership and self-rated wealth were also measured. In total, 596 cases and 535 controls were included in these analyses (Kenya 142 cases, 75 controls; Bangladesh 216 cases, 279 controls; Philippines 238 cases, 180 controls). Case <span class="yellow">participants</span> were more likely to be in the lowest quartile of per capita expenditure (PCE) compared to controls in Kenya (odds ratio = 2.3, 95% confidence interval 0.9–5.5), Bangladesh (1.9, 1.1–3.2), and the Philippines (3.1, 1.7–5.7), and there was significant dose–response relationship across quartiles of PCE. These associations persisted after adjustment for self-rated health and social support indicators. A similar pattern was observed for the relationship between cataract visual impairment with asset ownership and self-rated wealth. There was no consistent pattern of association between PCE and level of visual impairment due to cataract, sex, or age among the three countries.<br><br>Conclusions
Our data show that <span class="yellow">people</span> with visual impairment due to cataract were poorer than those with normal sight in all three low-income countries studied. The MDGs are committed to the eradication of extreme poverty and provision of health care to poor <span class="yellow">people</span>, and this study highlights the need for increased provision of cataract surgery to poor <span class="yellow">people</span>, as they are particularly vulnerable to visual impairment from cataract.<br><br>Background.
Globally, about 45 million <span class="yellow">people</span> are blind. As with many other conditions, avoidable blindness (preventable or curable blindness) is a particular problem for <span class="yellow">people</span> in developing countries—90% of blind <span class="yellow">people</span> live in poor regions of the world. Although various infections and disorders can cause blindness, cataract is the most common cause. In cataract, which is responsible for half of all cases of blindness in the world, the lens of the eye gradually becomes cloudy. Because the lens focuses light to produce clear, sharp images, as cataract develops, vision becomes increasingly foggy or fuzzy, colors become less intense, and the ability to see shapes against a background declines. Eventually, vision may be lost completely. Cataract can be treated with an inexpensive, simple operation in which the cloudy lens is surgically removed and an artificial lens is inserted into the eye to restore vision. In developed countries, this operation is common and easily accessible but many poor countries lack the resources to provide the operation to everyone who needs it. In addition, blind <span class="yellow">people</span> often cannot afford to travel to the hospitals where the operation, which also may come with a fee, is done.<br><br>Why Was This Study Done?
Because blindness may reduce earning potential, many experts believe that poverty and blindness (and, more generally, poor health) are inextricably linked. <span class="yellow">People</span> become ill more often in poor countries than in wealthy countries because they have insufficient food, live in substandard housing, and have limited access to health care, education, water, and sanitation. Once they are ill, their ability to earn money may be reduced, which increases their personal poverty and slows the economic development of the whole country. Because of this potential link between health and poverty, improvements in health are at the heart of the United Nations Millennium Development Goals, a set of eight goals established in 2000 with the primary aim of reducing world poverty. However, few studies have actually investigated the complex relationship between poverty and health. Here, the researchers investigate the association between visual impairment from cataract and poverty among adults living in three low-income countries.<br><br>What Did the Researchers Do and Find?
The researchers identified nearly 600 <span class="yellow">people</span> aged 50 y or more with severe cataract-induced visual impairment (“cases”) primarily through a survey of the population in Kenya, Bangladesh, and the Philippines. They matched each case to a normally sighted (“control”) <span class="yellow">person</span> of similar age and sex living nearby. They then assessed a proxy for the income level, measured as “per capita expenditure” (PCE), of all the study <span class="yellow">participants</span> (<span class="yellow">people</span> with cataracts and controls) by collecting information about what their households consumed. The <span class="yellow">participants</span>' housing conditions and other assets and their self-rated wealth were also measured. In all three countries, cases were more likely to be in the lowest quarter (quartile) of the range of PCEs for that country than controls. In the Philippines, for example, <span class="yellow">people</span> with cataract-affected vision were three times more likely than normally sighted controls to have a PCE in the lowest quartile than in the highest quartile. The risk of cataract-related visual impairment increased as PCE decreased in all three countries. Similarly, severe cataract-induced visual impairment was more common in those who owned fewer assets and those with lower self-rated wealth. However, there was no consistent association between PCE and the level of cataract-induced visual impairment.<br><br>What Do These Findings Mean?
These findings show that there is an association between visual impairment caused by cataract and poverty in Kenya, Bangladesh, and the Philippines. However, because the financial circumstances of the <span class="yellow">people</span> in this study were assessed after cataracts had impaired their sight, this study does not prove that poverty is a cause of visual impairment. A causal connection between poverty and cataract can only be shown by determining the PCEs of normally sighted <span class="yellow">people</span> and following them for several years to see who develops cataract. Nevertheless, by confirming an association between poverty and blindness, these findings highlight the need for increased provision of cataract surgery to poor <span class="yellow">people</span>, particularly since cataract surgery has the potential to improve the quality of life for many <span class="yellow">people</span> in developing countries at a relatively low cost.<br><br>Additional Information.
Please access these Web sites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.0050244.<br><br><br><br>Introduction
Improvements in health are at the heart of the Millennium Development Goals, with the recognition that better health is central to the primary aim of reducing poverty as well as important in its own right. Empirical data are needed to back up this claim. Unravelling the relationship between blindness and poverty therefore has important implications, and may also be informative for the association between poverty and other disabilities.
Blindness is a common condition globally, affecting approximately 45 million <span class="yellow">people</span>, and more than a third of blindness is caused by cataract [1,2]. Globally, the prevalence of blindness is five-fold higher in poor than rich countries [2]. Limited data show that within countries the poor are also more likely to be blind [3,4]. It is frequently asserted that blindness is both a cause and consequence of poverty, but there are few empirical data to support this claim. Poverty may cause cataract blindness, because access to cataract surgery is limited in low-income countries [5]. Furthermore, within poor countries some evidence suggests that lack of money is a major barrier to uptake of cataract surgery by individuals [6–8]. Blindness may also cause poverty, as the blind individual, or the household members who care for them, have a reduced earning potential [4,9]. This complex problem could have serious implications; estimates from The Gambia suggest that there is a substantial economic burden from lost productivity among blind <span class="yellow">people</span> [10]. Therefore, blindness prevention may ultimately be cost saving [11]. Extrapolations on a global level indicate that a successful eye care programme could prevent more than 100 million cases of blindness between 2000 and 2020, and consequently save at least US$102 billion, which would otherwise be lost to reductions in productivity associated with blindness [12]. However, these estimates are based on extrapolations from limited data and were not based on individual-level data. It is also difficult to identify the component of productivity loss that is due to blindness, as this condition mainly affects older <span class="yellow">people</span>, who may suffer from other comorbidities that restrict their employment opportunities or make them dependent on the care of others.
The Cataract Impact Study was undertaken to assess the relationship between cataract visual impairment and “economic poverty” and quality of life, and to estimate the impact of cataract surgery on these factors in three low-income countries. The aim of the current paper is to assess the association at baseline between visual impairment from cataract and household poverty (measured through consumption, asset ownership, and self-rated wealth) in a population-based case–control study in Kenya, the Philippines, and Bangladesh.<br><br>Methods
Setting
Case and control <span class="yellow">participants</span> were recruited from Nakuru district, Kenya (January–February, 2005); Negros island (May–June, 2005) and Antique district (April–May, 2006), Philippines; and Satkhira district, Bangladesh (November–December, 2005).<br><br>Selection of Cases and Controls
<span class="yellow">Persons</span> with cataract visual impairment (cases) and <span class="yellow">persons</span> without (controls) were primarily recruited through a population-based survey of adults aged ≥ 50 y [6–8]. Clusters of 50 <span class="yellow">people</span> (regardless of visual impairment) aged ≥ 50 y were selected through probability-proportionate to size sampling, using either the census (Philippines and Bangladesh) or electoral role (Kenya) as the sampling frame. Households within clusters were selected through a modification of compact segment sampling, whereby a map was drawn of the enumeration area that was divided into segments, each including approximately 50 <span class="yellow">people</span> aged ≥ 50 y, and one segment was chosen at random [13]. Households in the segment were included sequentially until 50 <span class="yellow">people</span> aged ≥ 50 y were identified. The surveys included 3,503 (93% response rate) <span class="yellow">people</span> aged ≥ 50 y in Kenya, 4,868 (92%) in Bangladesh, 2,774 (76%) in Negros, and 3,177 (83%) in Antique.
All <span class="yellow">people</span> in the survey aged ≥ 50 y underwent visual acuity (VA) testing and ophthalmic examination. VA was measured in full daylight with available spectacle correction with a Snellen tumbling “E” chart using optotype size 6/18 (20/60) on one side and size 6/60 (20/200) on the other side at 6 or 3 metres. If the VA was <6/18 in either eye then pinhole vision was also measured. <span class="blue">Participants</span> with pinhole vision <6/18 but >6/60 in the better eye due to age-related cataract were given a second VA test using an “E” of size 6/24. The ophthalmologist examined all eyes with a presenting VA <6/18 with a torch (i.e., flashlight), direct ophthalmoscope, and/or portable slit lamp. The principal cause of blindness or visual impairment was recorded, according to the WHO convention in which the major cause is assigned to the primary disorder or, if there are two existing primary disorders, to the one that is easiest to treat [14].
Survey <span class="yellow">participants</span> were eligible for inclusion as cases if they were aged ≥ 50 y with best corrected visual acuity <6/24 in the better eye due to cataract, as diagnosed by an ophthalmologist. All eligible cases identified from these surveys were invited to participate in the study. <span class="blue">Participants</span> were eligible to be controls if they were aged ≥ 50 y, did not have VA <6/24 in the better eye due to cataract and did not live in the same household as a case. During the survey a list was maintained of all eligible controls, by age group (50–54, 55–59, 60–64, 65–69, and >70) and sex. Whenever a case was identified, one age- and sex-matched control was randomly selected from the list for inclusion (or up to two controls in Bangladesh). If no matching eligible controls had been identified in that cluster at that stage of the survey, then the next eligible control in the cluster was recruited.
Because of logistical and time constraints, additional cases were also included through community-based case detection. In Kenya and Negros (Philippines), clusters were randomly selected through probability proportionate to size using the same cluster sampling procedure after completion of the population-based survey. Clusters were visited in advance and asked that all <span class="yellow">people</span> ≥ 50 y with vision problems come to a central point on a specified day, and that a list be made of <span class="yellow">people</span> unable to attend (e.g., due to blindness or other physical disability). After examining <span class="yellow">patients</span> at the central point, the survey team then visited those unable to leave their houses. Any identified eligible cases that agreed to be part of the study were interviewed in their homes. In Bangladesh and Antique (Philippines), community case detection was carried out simultaneously with the survey by two of the four teams, so that controls were included for these cases. Within each cluster from the survey, one interviewer was asked to be taken to two community members aged ≥ 50 y with eye problems, living within the cluster boundaries but not from the segments selected for the survey. If VA was <6/24 with pinhole in the better eye, the ophthalmologist was called to carry out the full eye examination, and eligible cases were included in the study.
For the purposes of the present analyses, control individuals with any visual impairment (VA <6/18 in the better eye) were excluded (n = 14 in Kenya, n = 53 in Bangladesh, n = 24 in the Philippines). Case and control <span class="yellow">participants</span> who were significantly communication impaired (e.g. deafness, dementia, or psychiatric disease) were excluded (fewer than five per country), and one case was excluded in the Philippines because of missing age data. One household had two eligible cases (Kenya), and one of these <span class="yellow">participants</span> was excluded for the poverty analyses as poverty was assessed through household level indicators (see below).
In total, 147 cases (82 from the survey and 65 from case detection) and 79 controls were included in Kenya; 217 cases (162 from survey and 55 from case detection) and 280 controls in Bangladesh; and 238 cases (146 survey and 92 case detection) and 180 controls in the Philippines.<br><br>Data Collection
All case and control <span class="yellow">participants</span> were interviewed in their homes by trained interviewers in the local language. Each interview lasted approximately 1 h.
Measures of poverty.
Poverty was measured through (a) monthly per capita expenditure (PCE) to indicate consumption, (b) asset ownership, and (c) self-rated wealth. The economic part of the questionnaires was adapted through interviews, focus group discussions, and pilot testing in each country to ensure local relevance.
The <span class="yellow">person</span> primarily responsible for household finances (which may have been the case/control or another household member) was interviewed to assess PCE and assets. PCE was measured using methods based on the World Bank's Living Standards Measurement Study [15]. Items were included on food (42–52 items per country), education (three items), health (five items), household expenses (nine items), and personal expenses (21 or 22 items). In total, 85 items were included in the questionnaire in Kenya, 90 in the Philippines, and 79 in Bangladesh. The informant was asked to recall the monetary value of food that was purchased, consumed from home production, or received as payment in kind or as gifts. Consumption was assessed over a 1-wk period for frequently consumed items, and this was scaled up to estimate monthly consumption. The amount consumed monthly was assessed for items that were consumed more rarely. Monthly rent was recorded among households who rented, and households who owned their property were asked to estimate the amount that they could charge in rent per month. The consumption on all items was summed to calculate total monthly household consumption, and this was converted to United States dollars (US$) at the 2005 exchange rate ($1 = 76 Kenya shillings, 64 Bangladesh taka, 55 Philippine pesos). Total monthly household consumption was divided by the number of household members to calculate monthly PCE for the household.
The household informant was also asked about the number and type of context-specific assets owned by the household, including different types of furniture, electrical equipment, <span class="yellow">cattle</span>, and vehicles. Information was collected on household characteristics, including the building material of the floor, roof, and walls; type of toilet; and the number of rooms.
Self-rated wealth was assessed by asking the household informant to rank the household's wealth relative to others in the community on a scale from 1 (poorest) to 10 (richest).<br><br>Covariates.
Case and control individuals were interviewed about standard sociodemographic indicators, including household composition, education, and employment. Information was collected on vision-related quality of life using the World Health Organization Prevention of Blindness and Deafness 20-item Visual Functioning Questionnaire [16,17], and health-related quality of life was assessed using items from the European Quality of Life Questionnaire [18]. Detailed time-use data were collected using methods based on the World Bank's Living Standards Measurement Study [15].<br><br>
Training and Fieldwork
Interviewers were trained for 1 wk, including 2 d of pilot testing. Attempts were made to minimise measurement bias by emphasising the need for consistency in data collection among cases and controls. The questionnaires were translated into the local languages (three in Kenya, three in the Philippines, and one in Bangladesh) and back-translated by independent translators (one for each language) who were also asked to comment on appropriateness of language used for the target population. A review was held to discuss differences in translation and modify accordingly. The questionnaire was piloted in each setting and small modifications to wording of some items were made, where appropriate, to ensure local understanding. Teams were accompanied by a field supervisor at least 1 d per wk to ensure that high quality was maintained and interviews were observed randomly throughout the study.<br><br>Statistical Analysis
Microsoft Access was used for data entry, and all data were double entered and validated. Analyses were undertaken in SAS version 8.2.
The mean and range of each expenditure item was calculated to assess whether answers were plausible, and to identify and exclude gross outliers (none identified). Rental equivalents were imputed based on household characteristics and non-rent expenditure for households where these estimates were missing or unreasonably low (< $1 per mo) (four in Kenya, three in Bangladesh, 18 in the Philippines). Total monthly household consumption was divided by the number of household members to calculate per capita household expenditure. Per capita household expenditure was divided into quartiles, separately for each country, based on the distribution of the data for the case and control <span class="yellow">participants</span> combined. Households with incomplete expenditure data were excluded from analyses (five cases and four controls in Kenya; one case and one control in Bangladesh).
A relative index of household assets was derived using principal components analysis (PCA) to determine weights for a list of assets and wealth indicators [19]. Variables entered into the PCA included building materials of the house, ownership of ten household assets, animal ownership, and education of the head of the household. The derived index was divided into quartiles from poorest (lowest socioeconomic status [SES] index) to least poor (highest SES index). PCA analyses were undertaken separately for each country. The means of the poverty variables were first compared for cases recruited through the two different methods, and then from cases and controls using t-tests for continuous variables (e.g., PCE and assets). For categorical variables (e.g., household rank) we used the Mann-Whitney test and presented medians and interquartile ranges. PCE was highly skewed and therefore was log transformed for the t-tests. The two-way correlations were calculated between PCE, assets, and household rank, in turn.
Logistic regression analyses were undertaken separately for each country, assessing the association between case/control status and sociodemographic and poverty variables. Conditional logistic regression was not undertaken, since the matching was incomplete, so all analyses were adjusted for the matching variables (age, sex, and rural/urban location). Likelihood ratio tests were undertaken to assess the significance of adding covariates with more than two levels (e.g., age groups, self-rated health groups) to the model. Tests for trend were undertaken across quartiles of the poverty variables and assessed using the p-value for trend. Analyses were also conducted adjusting for the logistic regression analyses for poverty by social support indicators (marital status and household size) and self-rated health, since these variables may confound the association between cataract visual impairment and poverty. Analyses from the Philippines were also adjusted for study site, since data were obtained from two settings (Negros and Antique). An attempt was made to disentangle the relationship between poverty and cataract by stratifying the analyses by age, sex, and level of visual impairment among the cases.<br><br>Ethical Approval
Informed signed or thumb-printed consent was obtained from all cases and controls. In Kenya and Bangladesh all cases were offered free cataract surgery at the local hospital, with free transport. In the Philippines, <span class="yellow">patients</span> were referred for surgery, which was subsidised for <span class="yellow">patients</span> who could not afford the fee. Ethical approval for this study was obtained from the ethics committees of the London School of Hygiene & Tropical Medicine, the Kenya Medical Research Institute, the Bangladesh Medical Research Council, and the University of St. La Salle, Bacolod, Philippines. This study complied with the guidelines of the Declaration of Helsinki.<br><br>
Results
Sociodemographic Characteristics of Cases and Controls
Case and control <span class="yellow">participants</span> were matched reasonably closely by sex and location. However, within the age category ≥ 70 y, cases tended to be older than the controls, so that cases were over-represented in the oldest age groups (75–79 and ≥ 80 y) compared to controls (Table 1). Cases were less likely to be married than controls, in Kenya (OR 0.6, 95% CI 0.3–1.1), Bangladesh (0.6, 0.4–1.0), and the Philippines (0.7, 0.4–1.0), although this only reached statistical significance in Bangladesh (p = 0.03). There was a strong protective effect of literacy and education on cataract in Bangladesh and Kenya that was not evident in the Philippines. Cases were substantially less likely to have a job other than working in the field compared to controls in all three countries. Cases reported significantly poorer self-rated health than controls—this pattern was particularly evident in the Philippines (OR for lowest versus highest quartile of self-rated health = 5.7, 95% CI 3.0–10.7) but also apparent in Kenya (2.6, 1.1–6.2) and Bangladesh (3.3, 2.1–5.3).<br><br>Summary Wealth Measures
All three settings were poor. The mean PCE was less than US$1 per <span class="yellow">person</span> per day in all three settings: US$26.4 (standard deviation [SD] = US$34.9) in Kenya, US$21.7 (US$48.0) in Bangladesh and US$26.1 (US$23.5) in the Philippines. The biggest expense was food in all three settings, making up 55% of PCE in Kenya, 47% in Bangladesh, and 64% in the Philippines, followed by household expenses including rent (21% in Kenya, 28% Bangladesh, and 22% Philippines) (Figure 1). The majority of food consumption was from direct purchase (70% in Kenya, 75% in Bangladesh, and 77% in the Philippines) or home-grown production (24% in Kenya, 22% in Bangladesh, and 17% in the Philippines), and little was from gifts or payments.
An asset score was created through PCA in the three settings. The first principal component explained 22% of the variability in asset variables in Kenya, 25% in Bangladesh, and 24% in the Philippines. Self-perceived wealth of the household clustered around the average with a large proportion of households in Kenya (48%), Bangladesh (43%), and the Philippines (64%); households stating that they were ranked between 4 and 6, on a scale from 1 to 10, in terms of wealth in their community. The three measures of poverty were highly correlated, each showing significant correlation (p < 0.001) with the other measure.<br><br>Economic and Household Characteristics of Cases and Controls
There were no significant differences in PCE, assets, or household rank between cases recruited through the population-based survey and those recruited through case detection, with the exception that the case-detection cases had lower household rank in Kenya (mean = 3.7 versus 3.1, p = 0.02). Consequently, cases recruited through the two methods were combined in the subsequent analyses.
Cases were poorer than controls, in all three settings according to all three poverty measurements (Table 2). The mean PCE was 20%–28% lower for members of households with a case than for control households, and this difference was highly significant in Bangladesh and the Philippines; for Kenya it was lower but did not reach significance (p = 0.07). The PCA score for assets was significantly lower among cases than controls in Kenya and Bangladesh, and it was lower in the Philippines although it did not reach significance (p = 0.06). Self-perceived wealth was significantly lower for households with a case compared to control households in Kenya (3.4 versus 4.5) and Bangladesh (3.9 versus 4.6), though not in the Philippines (4.1 versus 4.3).
There was no difference in the size of the households of cases and controls in any of the three settings. The ratio of dependents (i.e., household member aged <15 or ≥ 50 y) to independents (i.e., household member aged 15–50 y) was similar between cases and controls in Bangladesh (1.4 versus 1.4), but the dependency ratio was higher for controls than cases in Kenya (2.1 versus 1.6) and the Philippines (1.7 versus 1.3), due to the smaller number of <span class="yellow">people</span> of working age.<br><br>Patterns of Expenditure in Cases and Controls
Figure 1 shows the total PCE and the allocation of expenditure within quartiles of PCE for cases and controls. Monthly PCE was similar for cases and controls within each of the quartiles of expenditure. There was a gradual increase in PCE between the first three quartiles, and then a rapid increase between the third and the richest quartile. Within the first three quartiles of PCE the majority of expenditure was on food. Substantial expenditure on non-food items was observed only in the highest quartile of expenditure, where about half of expenditure was on non-food items. Similar patterns of PCE were observed for cases and controls in Kenya, Bangladesh, and the Philippines within each quartile of expenditure. These results demonstrate that cataract visual impairment was related to reduced PCE, but not allocation of expenditure.<br><br>Multivariate Analyses of Poverty and Cataract Visual Impairment
Multivariate analyses showed that case <span class="yellow">participants</span> were consistently poorer than controls in Kenya, Bangladesh, and the Philippines, using three different measures of poverty (Table 3). Cases were more likely than controls to be in the lowest quartile of PCE rather than the highest quartile in Kenya (OR 2.3, 95% CI 0.9–5.5), Bangladesh (1.9, 1.1–3.2) and the Philippines (3.1, 1.7–5.7). In all three settings these associations showed significant dose–response as assessed by the p-value for trend across the quartiles, with decreasing PCE related to case status and these relationships persisted after adjustment for self-rated health and social support indicators. A similar pattern was observed for the relationship between case–control status and asset ownership. Cases were significantly more likely to be in the lowest quartile of asset ownership rather than the highest quartile compared to controls in Kenya (3.7, 1.4–9.6), Bangladesh (2.6, 1.5–4.4), and the Philippines (2.1, 1.1–3.8). Cases were also significantly more likely to be in the lowest quartile of household rank rather than the highest, compared to controls in Kenya (3.5, 1.5–8.0), Bangladesh (2.7, 1.6–4.7) and the Philippines (2.3, 1.1–4.8). The associations with assets and household rank also showed a significant dose–response relationship, and the associations were largely unchanged after adjustment for self-rated health and social support indicators. In Kenya and Bangladesh the relationship between PCE and case status was somewhat weaker than for the other measures of poverty, while the reverse was true in the Philippines.
Stratifying the association between PCE and cataract visual impairment by level of visual impairment showed an inconsistent pattern (Table 4). In Kenya, the association with low PCE was somewhat stronger comparing cataract blind cases to controls (OR 3.1, 95% CI 0.9–10.8) than comparing moderate visually impaired cases to controls (1.8, 0.6–5.4), while this pattern was reversed in Bangladesh (blind cases versus controls: 1.8, 1.0–3.4; moderately visually impaired cases versus controls: 3.1, 1.3–7.2). In the Philippines the association with low PCE was strongest comparing severely visually impaired cases to controls (5.9, 2.0–17.6). The association between cataract visual impairment and PCE was stronger among <span class="yellow">men</span> than <span class="yellow">women</span> in Bangladesh and the Philippines, while the reverse was true in Kenya (Table 5). In Kenya and the Philippines the strongest association between cataract and PCE was among <span class="yellow">people</span> aged 70–79 y, while in Bangladesh the strongest effect was in <span class="yellow">people</span> aged over 80 y. Stratifying the association between assets and household rank with cataract by level of visual impairment, sex, or age broadly repeated these findings, and generally supported the lack of consistent pattern (unpublished data).<br><br>
Discussion
This large, multicentre population-based case–control study provides evidence that <span class="yellow">people</span> with visual impairment from cataract are poorer than control <span class="yellow">participants</span> with normal vision matched for age and sex. This pattern was evident whether poverty was measured in terms of PCE, assets, or self-rated wealth. Marital status seemed to be protective for cataract visual impairment, possibly indicating the role of social support in health-seeking behaviour. Reduced self-rated health was also strongly related to cataract visual impairment. This demonstrates the impact of poor vision on overall assessments of health and supports our previous finding of a relationship between cataract and quality of life [17].
Adjustment for marital status and self-rated health did not entirely explain the association between poverty and cataract visual impairment, suggesting that it operated through other pathways. Visual impairment could cause poverty through reduced employment opportunities. We might therefore expect to see a stronger relationship between cataract and poverty among the blind case <span class="yellow">participants</span> who may have fewer employment opportunities than among those less impaired (i.e., moderate visual impairment). Poverty may also cause visual impairment through restricted access to cataract surgery. In this case we would expect to see a stronger relationship between poverty and less severely affected cases (i.e., moderate visual impairment), as poor families may allocate money for surgery on members who are blind from cataract, so that poverty mainly restricts access to surgery among <span class="yellow">people</span> who are moderately visually impaired. The relationships that we observed between level of visual impairment and cataract were inconsistent across the three settings. Perhaps this shows that both pathways were operating or that the dynamics of the relationship between poverty and blindness vary in different settings. Levels of literacy and education were lower among cases than controls. These long-term indicators of disadvantage are unlikely to have changed after the onset of cataract. This observation provides some evidence that poverty preceded blindness in our study <span class="yellow">participants</span>.
It is frequently asserted that blindness is both a cause and consequence of poverty, but there are few empirical data to support this claim. Globally, the prevalence of blindness is five-fold higher in poor than rich countries [2], and data from Pakistan and India suggest that within countries the poor are more likely to be blind [3, 4]. Some blinding conditions are a direct consequence of poverty, notably trachoma, which thrives in poor areas lacking water and sanitation [20]. Other blinding diseases clearly contribute to poverty, such as onchocerciasis, which results in the abandonment of the fertile areas near to the rivers where the disease vector thrives [9]. A larger literature shows that poor <span class="yellow">people</span> are more likely to be ill or disabled than their richer compatriots, ranging from general disability in India, Bulgaria, and Ghana [21]; common mental disorders in Brazil, Chile, India, and Zimbabwe [22]; deafness in Brazil [23]; and tuberculosis in China [24]. There are also some exceptions such as a case-control study in Rwanda which failed to show an association between PCE and musculoskeletal impairment, perhaps because the population was almost universally poor [25].
Poverty may increase the incidence of disease, particularly preventable diseases such as tuberculosis. Poverty may also restrict access to appropriate health care and so prolong the duration of disease. A study in rural Tanzania showed that care-seeking behaviour for childhood illness is worse among poorer families than among the relatively rich families [26]. Another Tanzanian study found that <span class="yellow">people</span> with higher levels of asset ownership were more likely to obtain antimalarials even though they were less likely to be parasitaemic [27]. With respect to cataract, there is little evidence that prevention is possible, and so the main pathway from poverty to blindness is likely to be through reduced access to cataract surgical services. High health care costs may also exacerbate poverty. A study in rural China showed that ill health increases medical expenditure significantly, which detracts from expenditure on food, education, investment in farming, and participation in social activities [28]. Inability to afford cataract surgery is cited as the major barrier to the uptake of surgery in the surveys conducted in Kenya, the Philippines, and Bangladesh [6–8]. This indicates that the cost of surgery is perceived as substantial by many households, notwithstanding the problems of assessing the complex issue of barriers in the absence of in-depth qualitative interviews. Consequently, there are lower rates of cataract surgery among the poor [3].
Poverty may also limit the employment opportunities of the <span class="yellow">person</span> with disability or their household members. This pattern has been demonstrated for <span class="yellow">people</span> with <span class="yellow">HIV</span> in South Africa [29], tuberculosis in China [24], or disability in Sri Lanka [22]. An impact of blindness on reduced employment or income has been observed in Guinea [9] and India [4]. A belief that blindness reduces the employment opportunities of household members is widespread, but so far there is limited supportive evidence. There is a further complication to investigations of the relationship between cataract and poverty, as the individuals with cataract are likely to be elderly and facing multiple disabilities. Our study took account of the potential impact of multiple disabilities, as we adjusted for self-rated health, which is closely related to overall health, and this adjustment had no overall impact on our results [30].
Study Strengths
This was a large population-based case–control study, conducted in three countries, allowing international comparisons. This was the first study, to our knowledge, to relate PCE to visual impairment. We also measured assets, which reflects long-term access to resources, and self-rated wealth. We used expenditure as a proxy for income, which has aided both academic and nonacademic investigations. As one example, the notorious Chicago gangster Al Capone managed to escape prosecution for smuggling, gambling, bootlegging, and murder for years, but was eventually convicted of tax evasion, because the jury was convinced that his exorbitant expenses on clothes, furnishing, foods, and gifts were inconsistent with his claim that he had no income. Expenditure often provides a better measure of poverty than income for a number of reasons. Income may be variable by season, whereas households attempt to smooth expenditure over the year. <span class="yellow">People</span> are more comfortable sharing information about expenditure than income, and it may be a more meaningful measure than income in an agrarian society as it reflects what the household is able to command based on its current income, borrowing ability, or household savings [31]. PCE also has advantages over assets, as it may be more responsive to change, which will be important for the follow-up analyses of the study <span class="yellow">participants</span> after they have undergone cataract surgery.<br><br>Study Limitations
There are a number of limitations relating to the measurement of poverty in this study. Our analyses focus on monetary indicators of poverty, while we acknowledge that health, education, and housing are also important. We concede that it is difficult to measure expenditure accurately [32,33], but this also true for the measurement of diet and other variables, which is standard practise in many epidemiological studies. Furthermore, a large number of items were included in our measure of expenditure so that the measure was comprehensive [33]. Expenditure data were not validated through diaries or other means, although assets and self-rated wealth correlated highly with PCE. Other recent estimates of expenditure are not available from surveys conducted in these countries to allow comparison. The per capita estimates of monthly gross national income from the World Development Indicators database show somewhat higher estimates in Kenya (US$48) and Bangladesh (US$40) than our PCE derived estimates, and far higher estimates for the Philippines (US$108). This discrepancy may be reasonable, as the World Development Indicators reflect national averages, while we sampled the households with elderly <span class="yellow">people</span> in poor regions of the country, many of whom were visually impaired from cataract. PCE was calculated simply by dividing the total household expenditure by the number of household members, without inclusion of economies of scale or equivalence scales. There is no widely accepted alternative to the simple equal-sharing convention, and the majority of expenditure was on food which does not allow for economies of scale. Furthermore, there were slightly fewer <span class="yellow">people</span> of working age in the control households in Kenya and the Philippines, so adjustment for equivalence scores would be unlikely to explain the higher poverty among cases. The case and control households were of similar sizes in the three settings, so economies of scales are unlikely to have explained the differences.
There were a number of limitations relating to study design. Unfortunately, we did not record the exact numbers of cases and controls who refused to participate or were unable to communicate (believed to be fewer than five in each country), so the response rate is unknown, but was believed to be high. A variety of methods were used for case recruitment, as we were not able to obtain enough cases through the survey alone. However, cases recruited through the population-based survey and through case detection had similar poverty characteristics.<br><br>Conclusions
Our data show that <span class="yellow">people</span> with visual impairment due to cataract were poorer than controls in three low income countries, Bangladesh, Kenya, and the Philippines. The Millennium Development Goals are committed to the eradication of extreme poverty and provision of health care to poor <span class="yellow">people</span>. This study confirms an association between poverty and blindness and highlights the need for increased provision of cataract surgery to poor <span class="yellow">people</span>, particularly since cataract surgery is a highly cost-effective intervention in these settings [34].<br><br><br><br><h3>pmcA2636797</h3>Efficacy of intra-articular hyaluronan (Synvisc®) for the treatment of osteoarthritis affecting the first metatarsophalangeal joint of the foot (hallux limitus): study protocol for a randomised placebo controlled trial
Abstract
Background
Osteoarthritis of the first metatarsophalangeal joint (MPJ) of the foot, termed hallux limitus, is common and painful. Numerous non-surgical interventions have been proposed for this disorder, however there is limited evidence for their efficacy. Intra-articular injections of hyaluronan have shown beneficial effects in case-series and clinical trials for the treatment of osteoarthritis of the first metatarsophalangeal joint. However, no study has evaluated the efficacy of this form of treatment using a randomised placebo controlled trial. This article describes the design of a randomised placebo controlled trial to evaluate the efficacy of intra-articular hyaluronan (Synvisc®) to reduce pain and improve function in <span class="yellow">people</span> with hallux limitus.<br><br>Methods
One hundred and fifty community-dwelling <span class="yellow">men</span> and <span class="yellow">women</span> aged 18 years and over with hallux limitus (who satisfy inclusion and exclusion criteria) will be recruited.
<span class="blue">Participants</span> will be randomised, using a computer-generated random number sequence, to receive a single intra-articular injection of up to 1 ml hyaluronan (Synvisc®) or sterile saline (placebo) into the first MPJ. The injections will be performed by an interventional radiologist using fluoroscopy to ensure accurate deposition of the hyaluronan in the joint. <span class="blue">Participants</span> will be given the option of a second and final intra-articular injection (of Synvisc® or sterile saline according to the treatment group they are in) either 1 or 3 months post-treatment if there is no improvement in pain and the <span class="yellow">participant</span> has not experienced severe adverse effects after the first injection. The primary outcome measures will be the pain and function subscales of the Foot Health Status Questionnaire. The secondary outcome measures will be pain at the first MPJ (during walking and at rest), stiffness at the first MPJ, passive non-weightbearing dorsiflexion of the first MPJ, plantar flexion strength of the toe-flexors of the hallux, global satisfaction with the treatment, health-related quality of life (assessed using the Short-Form-36 version two questionnaire), magnitude of symptom change, use of pain-relieving medication and changes in dynamic plantar pressure distribution (maximum force and peak pressure) during walking. Data will be collected at baseline, then 1, 3 and 6 months post-treatment. Data will be analysed using the intention to treat principle.<br><br>Discussion
This study is the first randomised placebo controlled trial to evaluate the efficacy of intra-articular hyaluronan (Synvisc®) for the treatment of osteoarthritis of the first MPJ (hallux limitus). The study has been pragmatically designed to ensure that the study findings can be implemented into clinical practice if this form of treatment is found to be an effective treatment strategy.<br><br>Trial registration
Australian New Zealand Clinical Trials Registry: ACTRN12607000654459<br><br><br><br>Background
Osteoarthritis (OA) is a degenerative joint disease that commonly presents within the first metatarsophalangeal joint (MPJ) of the foot. The terms hallux limitus and hallux rigidus have frequently been used interchangeably to describe differing severities of pain and limitation of motion associated with OA at the first MPJ [1]. Hallux limitus is a progressive osteoarthritic condition of the first MPJ that may advance to an end-stage presentation of hallux rigidus where the joint fuses and there is a complete restriction of motion [1]. First MPJ OA is the second most common disorder affecting the foot after hallux valgus [2]. The prevalence of the condition increases with age, and it has been reported that radiographic changes in the first MPJ affect are evident in approximately 46% of <span class="yellow">women</span> and 32% of <span class="yellow">men</span> at 60 years of age [3]. Osteoarthritis at the first MPJ is characterised by the symptoms of pain and stiffness at the joint [1]. Secondary painful symptoms relate to compensations during gait that may occur due to the reduced motion of the first MPJ [1]. The presence of pain associated with first MPJ OA impacts on normal walking and quality of life [4].
Treatment of hallux limitus involves conservative measures (such as physical therapy, foot orthoses, footwear modification, joint manipulation and injection with corticosteroid) [5], or surgical intervention (either joint-salvage or joint-destructive procedures) [6]. Pharmacological treatment is also often undertaken as an adjunct for pain relief in the management of hallux limitus [6]. However, although non-steroidal anti-inflammatory drugs (NSAIDs) and cyclooxygenase-2 inhibitors have been found to be effective in the management of various forms of OA, gastrointestinal complications remain a concern [7]. In light of these limitations with existing treatments, an alternative treatment termed 'viscosupplementation' – the intra-articular injection of hyaluronan into arthritic joints with the aim of restoring the viscoelasticity of the synovial fluid [8] – has been proposed and has attracted considerable attention in the medical literature as a treatment for OA [9]. In particular, both the American College of Rheumatology (ACR) and European League Against Rheumatism (EULAR) recommend hyaluronan in the management of OA of the knee [10,11]. Although the results of systematic reviews investigating the effectiveness of this type of treatment for knee OA are controversial, the most recent update of the Cochrane systematic review evaluating viscosupplementation for the treatment of knee OA concluded that viscosupplementation was both safe and effective for the treatment of OA and was superior or equivalent to any form of systemic intervention or intra-articular corticosteroids [9,12].
Despite there being a large number of studies investigating the effectiveness of hyaluronan for knee OA, few studies have investigated the effects of this form of treatment for OA at the first MPJ [13]. In a case-series retrospective study, 14 <span class="yellow">patients</span> with radiographically confirmed OA at the first MPJ that received up to 3 intra-articular injections of 1 ml hyaluronan (Ostenil® Mini) (sodium hyaluronate) reported a statistically significant reduction in pain (reported using a visual analogue scale) after 6 months [14]. The treatment was well tolerated, with 3/14 (21%) <span class="yellow">participants</span> reporting mild adverse reactions at the injection site. In another study, Pons et al[13] compared a single intra-articular injection of 1 ml Ostenil® Mini (sodium hyaluronate) with 1 ml Trigon depot® (triamcinolone acetonide, a corticosteroid) for the treatment of painful, grade 1 hallux limitus (Karasick and Wapner [15] scale) in 37 <span class="yellow">participants</span> (40 feet) [13]. Both treatment groups showed statistically significant reductions in pain at rest or on palpation for up to 12 weeks post-injection. However, hyaluronan treatment resulted in a statistically significant greater reduction in pain during walking and greater improvement in the American Orthopaedic Foot and Ankle Society (AOFAS) hallux MPJ score compared to treatment with triamcinolone acetonide. The treatment with hyaluronan was well tolerated, with 2/20 (10%) <span class="yellow">participants</span> reporting mild adverse reactions at the injection site.
Although both of these studies suggest that intra-articular hyaluronan is safe and effective for the treatment of hallux limitus, neither used a placebo control group [13,14]. This limitation is significant as a placebo effect can account for 79% of the efficacy of intra-articular hyaluronan treatment [16]. Further, both studies are limited in that neither of the studies used blinding of both the <span class="yellow">participants</span> and assessors in their protocols. It is therefore possible that the positive effects of hyaluronan may have been overestimated. Accordingly, the aims of this project are to conduct a double blind randomised controlled trial to determine the effectiveness of intra-articular hyaluronan (Synvisc®) on (i) foot pain and function; (ii) the range of motion of the first MPJ; (iii) the strength of the plantarflexor muscles of the first MPJ; (iv) the health related quality of life; and (v) the use of pain-relieving medications in <span class="yellow">people</span> with hallux limitus. The study protocol is presented in this paper, consistent with the recommendations of Editorial Board of BioMed Central [17].<br><br>Methods
Design
This study is a parallel group, <span class="yellow">participant</span> and assessor blinded, randomised controlled trial with a 6 month follow-up (Figure 1). It has been developed using the principles described by Osteoarthritis Research Society International (OARSI) Clinical Trials Task Force guidelines [18]. <span class="blue">Participants</span> will be randomised to receive a single intra-articular injection of up to 1 ml hyaluronan (Synvisc®) or sterile saline (placebo) into the first MPJ. Allocation to either the Synvisc® or placebo groups will be achieved using a computer-generated random number sequence. The allocation sequence will be generated and held by an external <span class="yellow">person</span> not directly involved in the trial. Concealment of the allocation sequence will be ensured as each <span class="yellow">participant</span>'s allocation will be contained in a sealed opaque envelope. Envelopes will be made opaque by using a sheet of aluminium foil inside the envelope. In addition, a system using carbon paper will be employed so the details (name and date of recruitment) are transferred from the outside of the envelope to the paper inside the envelope containing the allocation prior to opening the seal. Assessors and <span class="yellow">participants</span> will be blinded to group allocation. <span class="blue">Participants</span> will be given the option of a second and final intra-articular injection (of Synvisc® or sterile saline according to the treatment group they are in) on days 30 or 90 if there is no improvement in pain and the <span class="yellow">participant</span> has not experienced severe adverse effects after the first injection).<br><br><span class="blue">Participants</span>
The <span class="yellow">Human</span> Studies Ethics Committee at La Trobe University (Human Ethics Committee Application No. 07-45) and the Radiation Advisory Committee of the Victorian Department of <span class="yellow">Human</span> Services have given approval for the study. Written informed consent will be obtained from all <span class="yellow">participants</span> prior to their participation. <span class="yellow">People</span> with hallux limitus will be recruited from a number of sources:
(i) advertisements in relevant Melbourne (Australia) newspapers;
(ii) mail-out advertisements to health-care practitioners in Melbourne;
(iii) advertisements using relevant internet web-sites (including );
(iv) posters displayed in local retirement villages, community centres and universities located in Melbourne.
Respondents will initially be screened by telephone interview to ensure they are suitable for the study. Suitable individuals will then be invited to participate in the study and attend an initial assessment.
To be included in the study, <span class="yellow">participants</span> must meet the following inclusion criteria:
(i) be aged at least 18 years;
(ii) report having symptoms of pain, during walking or rest, in the first MPJ for at least 3 months;
(iii) report having pain rated at least 20 mm on a 100 mm visual analogue pain scale (VAPS);
(iv) have pain upon palpation of the dorsal aspect of the first MPJ;
(v) radiographic evidence of OA (score 1 or 2 for either osteophytes or joint space narrowing using a previously published radiographic classification) [19] at the first MPJ.
(vi) able to walk household distances (>50 meters) without the aid of a walker, crutches or cane;
(vii) be willing to attend the La Trobe University Medical Centre (Melbourne, Australia) for treatment with either Synvisc® or placebo (single intra-articular injection) and attend the Health Sciences Clinic at La Trobe University (Melbourne, Australia) for the initial assessment and the outcome measurements (at baseline and 1, 3 and 6 months post-treatment);
(viii) not receive other intra-articular injections into the first MPJ during the course of the study, apart from those dictated by the study;
(ix) be willing to discontinue taking all pain-relieving medications (analgesics and non-steroidal anti-inflammatory medications (NSAIDs), except paracetamol up to 4 g/day, taken by mouth or applied topically):
- for at least 14 days prior to the baseline assessment;
- during the study period (6 months after the final treatment with Synvisc®).
<span class="blue">Participants</span> who do take paracetamol need to discontinue its use at least 24 hours prior to the baseline assessment and follow-up assessments at 1, 3 and 6 months after the treatment;
(x) be willing to not receive any physical therapy on the involved MPJ or trial of shoe modifications or foot orthoses during the study period.
Exclusion criteria for <span class="yellow">participants</span> in this study will be:
(i) Severe radiographic evidence of OA (score 3 for either osteophytes or joint space narrowing) at the first MPJ using a previously published radiographic classification [19];
(ii) previous surgery on the first MPJ;
(iii) intra-articular steroid, or any other intra-articular injection at the first MPJ in the previous 6 months;
(iv) treatment with systemic steroid (excluding inhalation or topical steroids), immunosuppressives or anticoagulants (except for acetylsalicylic acid at dosages of up to 325 mg/day);
(v) presence of joint infection(s) of the foot;
(vi) significant deformity of the first MPJ including hallux abducto valgus (grade of 3 or 4 scored using the Manchester Scale [20];
(vii) presence of peripheral vascular disease. Peripheral vascular disease will be considered to be present if any of the following are present [21];
▪ past history of, vascular surgery, Raynaud's phenomenon, vasculitis associated with connective tissue diseases, Buerger's disease, arterial emboli, deep vein thrombosis or lower limb ulcers;
▪ history of intermittent claudication or rest pain;
▪ presence of atrophy, ulcers or significant oedema;
▪ inability to palpate at least one pedal pulse;
▪ Ankle Brachial Pressure Index <0.9;
(viii) presence of one or more conditions that can confound pain and functional assessments of the first MPJ, such as metatarsalgia, plantar fasciitis, pre-dislocation syndrome, sprains of the foot, Achilles tendinopathy, degenerative joint disease of the foot (other than the first MPJ) or painful corns and callus;
(ix) planning to undergo any surgical procedure or receive any injections, apart from those dictated by the study, at the involved first MPJ during the study period;
(x) presence of systemic inflammatory condition or infection, such as inflammatory arthritis, diagnosed with rheumatoid arthritis, ankylosing spondylitis, psoriatic arthritis, reactive arthritis, septic arthritis, acute pseudogout, or any other connective tissue disease;
(xi) evidence of gout or other musculoskeletal disease other than OA within the feet. Gout will be screened for using clinical history and physical assessment (painful joint, abrupt onset, swelling), radiographic assessment (asymmetrical joint swelling, subcortical cysts without erosion and tophi) as well as serum uric acid levels (hyperuricaemia = serum uric acid > mean + 2 SD from normal population) [22];
(xii) active skin disease or infection in the area of the injection site;
(xiii) any medical condition that, in the opinion of the investigators, makes the <span class="yellow">participant</span> unsuitable for inclusion (e.g., severe progressive chronic disease, malignancy, bleeding disorder, clinically important pain in a part of the musculoskeletal system other than the first MPJ, or fibromyalgia);
(xiv) pregnant or lactating <span class="yellow">women</span>, or <span class="yellow">women</span> who are of <span class="yellow">child</span> bearing age or have not undergone menopause (Synvisc® has not been tested in pregnant <span class="yellow">women</span> or <span class="yellow">women</span> who are nursing);
(xv) cognitive impairment (defined as a score of < 7 on the Short Portable Mental Status Questionnaire) [23];
(xvi) known hypersensitivity (allergy) to hyaluronan preparations, or to avian proteins, feathers or egg products;
(xvii) involvement in any clinical research study in the previous 3 months that could be considered to affect the results of this study.<br><br>Intra-articular injections for the treatment groups
<span class="blue">Participants</span> will be randomised to receive a single intra-articular injection of up to 1 ml of hyaluronan (Synvisc®; Genzyme Biosurgery, Genzyme Corporation, NJ, USA) or sterile saline (placebo) into the first MPJ. Each 2 ml ampoule of Synvisc® contains 16 mg of hylan G-F 20 (cross-linked hylan polymers; hylan A and B), 17 mg sodium chloride, 0.32 mg disodium hydrogen phosphate, 0.08 mg sodium dihydrogen phosphate monohydrate. The hyaluronan is extracted from <span class="yellow">chicken</span> combs and the purified material has an average molecular weight of 6,000 kDa.
The injections will be performed by the same experienced interventional radiologist (AEZ) using fluoroscopic imaging to ensure accurate deposition of the hyaluronan within the joint. As the Synvisc® is provided in ampoules that are labelled with the product name, it will not be possible to blind the injector, however this <span class="yellow">person</span> is not involved in generation of the allocation order, recruitment, assessment or data analysis. The intra-articular injection will be performed using a 21 gauge (0.80 × 19 mm) Surflo® (Terumo® Corp., Tokyo, Japan) winged infusion set under aseptic procedures. Either a dorso-lateral or dorso-medial approach for injection will be used at the discretion of the injector (depending on which approach provides minimum interference from the osteophytes at the first MPJ joint margins). No anaesthetic will be used. If the <span class="yellow">participant</span> has bilateral painful first MPJs, only one side (the most painful side) will be treated and used for data collection. The injector will record the volume of the agent that is injected.
<span class="blue">Participants</span> will be given the option of a second and final intra-articular injection (of Synvisc® or sterile saline according to the treatment group they are in) on days 30 or 90 if there is no improvement in pain (assessed using the VAPS for pain during walking or at rest) and the <span class="yellow">participant</span> has not experienced severe adverse effects after the first injection).<br><br>Assessments
Initial assessments
An initial assessment will be performed to determine the eligibility of <span class="yellow">participants</span> for this study. Demographic data will be collected including the age, gender, height and weight of <span class="yellow">participants</span>. Data will also be obtained concerning the presentation of symptoms (foot affected, duration of symptoms). If the <span class="yellow">participant</span> has bilateral painful first MPJs, the most painful side will be used for data collection and subsequent treatment. To establish eligibility for the study, <span class="yellow">participants</span> will undergo a clinical assessment, have one set of dorso-plantar and lateral weight-bearing x-rays taken of their feet to grade the severity of first MPJ OA as well as undergo a blood test to assess serum uric acid levels (to exclude gout).
Weightbearing dorso-plantar and lateral radiographic views will be obtained from both feet with the <span class="yellow">participant</span> standing in a relaxed bipedal stance position. All x-rays will be taken by the same medical imaging department using a Shimadzu UD150LRII 50 kw/30 kHz Generator and 0.6/1.2 P18DE-80S high speed x-ray tube from a ceiling suspended tube mount. AGFA MD40 CR digital phosphor plates in a 24 cm × 30 cm cassette will be used. For dorso-plantar projections, the x-ray tube will be angled 15° cephalad and centered at the base of the third metatarsal. For lateral projections, the tube will be angled 90° and centered at the base of the third metatarsal. The film focus distance will be set at 100 cm [19].<br><br>Baseline assessments and outcome measures
<span class="blue">Participants</span> who are eligible for the study will be invited to attend a baseline assessment. During the baseline assessment, <span class="yellow">participants</span> will undergo primary and secondary outcome measurements prior to receiving their injection. The outcome measurements have been developed in accordance of the recommendations of the OARSI Clinical Trials Task Force guidelines [18].<br><br>Primary outcome measures
Outcome measurements (primary and secondary) will occur at four time-points at baseline, 1, 3 and 6 months post-treatment (after the intra-articular injection of Synvisc® or placebo). The assessor performing the measurements will be blinded as to which treatment group <span class="yellow">participants</span> have been allocated to. <span class="blue">Participants</span> who receive a second treatment at day 30 or 90 will be followed for a further 30 days or 90 days respectively and undergo outcome measurements at 7 or 9 months respectively.
The primary outcome measures will be the Pain and Function subscales of the Foot Health Status Questionnaire (FHSQ) [24]. The FHSQ includes 13 questions that assess four domains of foot health, Foot pain, Foot function, Footwear and General foot health. The FHSQ has been subjected to an extensive validation (content, criterion and construct validity) process. It has a high test-retest reliability (intraclass correlation coefficients ranging from 0.74 to 0.92) and a high degree of internal consistency (Cronbach's α ranging from 0.85 to 0.88) [24]. Rigorous reviews have rated it as one of the highest quality foot health status measures currently available [25-27].<br><br>Secondary outcome measures
The secondary outcome measures will be:
(i) Severity of pain
Severity of pain at the first MPJ during walking, and during rest, over the past week will be assessed using a 100 mm visual analogue pain scale. The left side of the scale (0 mm) will be labelled "no pain" and the right side of the scale (100 mm) will be labelled "worst pain possible" for each question [25,28].<br><br>(ii) Severity and duration of stiffness at the first metatarsophalangeal joint
The severity of stiffness at the first MPJ during walking over the past week will be assessed using a 100 mm visual analogue scale. The left side of the scale (0 mm) will be labelled "not stiff at all" and the right side of the scale (100 mm) will be labelled "most stiff possible". The average duration of stiffness at the first MPJ over the past week will be assessed using a four category scale response. The responses are: "none", "1–15 minutes", "16–30 minutes" and "greater than 30 minutes" [29].<br><br>(iii) Passive, non-weightbearing dorsiflexion range of motion of the first metatarsophalangeal joint
First MPJ dorsiflexion range of motion will be measured using a goniometer as the maximum angle at which the hallux cannot be passively moved into further extension in a non-weightbearing position (Figure 2) [30]. The test will be performed two times and the average will be used for analysis. This measurement technique shows high intra-reliability (ICC = 0.95, standard error of mean = 1.3°) [30].<br><br>(iv) Plantar flexion strength of the toe-flexors of the hallux
Plantar flexion strength of the toe-flexors of the hallux will be measured using the Mat Scan® plantar pressure measurement device [31]. <span class="blue">Participants</span> will be seated with the hip, knee, and ankle at 90 degrees and their foot placed over the Mat Scan® plantar pressure measurement device (Tekscan, Boston, MA, USA) (Figure 3a). This system consists of a 5-mm thick floor mat (432 × 368 mm) incorporating 2288 resistive sensors (1.4 sensors/cm2) sampling at a rate of 40 Hz. The mat will be calibrated for each <span class="yellow">participant</span> using his or her own bodyweight before each testing session. <span class="blue">Participants</span> will be instructed to use their toe-flexor muscles to maximally push their hallux down on the MatScan® device and forces under the hallux will be recorded (Figure 3b). The test will be performed three times for the hallux and the maximal force will be used for analysis. The test-retest reliability of this measurement technique has previously been shown to be high, with intraclass correlation coefficients (ICCs) = 0.88 (95% CI 0.81 – 0.93) [31].<br><br>(vi) Plantar pressure measurement
Plantar pressures will be recorded during level barefoot walking using the MatScan® system (Tekscan®, Boston, MA, USA). The two-step gait initiation protocol will be used to obtain foot pressure data, as it requires fewer trials than the mid-gait protocol and has similar re-test reliability [32]. Three trials will be recorded, which has been found to be sufficient to ensure adequate reliability of pressure data [32,33]. Following data collection, the Research Foot® software (version 5.24) will be used to construct individual "masks" to determine maximum force (kg) and peak pressure (kg/cm2) under seven regions of the foot: hallux, lesser toes, 1st MPJ, 2nd MPJ, 3rd to 5th MPJs, midfoot and heel (Figure 4a). For each region, the median of the three trials will be used for analysis. Typical plantar pressure recordings from a <span class="yellow">participant</span> are shown in Figure 4b.<br><br>(vi) Global satisfaction with the treatment
Global satisfaction with the treatment will be assessed using a 5-point Likert scale, as well as a dichotomous (yes/no) scale. The five point-Likert scale will ask "How satisfied are you with the treatment you received for your big-toe joint pain?", and will have the following five responses: "Dissatisfied", "Only moderately satisfied", "Fairly satisfied", "Clearly satisfied" and "Very satisfied". The dichotomous scale of satisfaction will be answered as "Yes"' or "No" in response to the question: "Would you recommend the treatment that you received to someone else with big-toe joint pain".<br><br>(vii) Health related quality of life
The Short-Form-36 (version two) (SF-36) questionnaire will be used to assess health related quality of life. The SF-36 is a 36 question survey that measures eight health concepts most affected by disease and treatment. The eight health concepts can then be used to form two summary measures: Physical health and Mental health. The Short Form-36 (SF-36) has been extensively validated and is one of the most widely used instruments to measure health status. The SF-36 shows content, concurrent, criterion, construct, and predictive evidence of validity. The reliability of the eight concepts and two summary measures has been assessed using both internal consistency and test-retest methods. Reliability statistics have exceeded 0.80 [34-37].<br><br>(viii) Self-reported magnitude of symptom change
Self-reported magnitude of symptom change will be measured using a 15-point Likert scale. The scale will ask <span class="yellow">participants</span> "how much have your symptoms in your big-toe joint have changed from the beginning of the study to now?". The fifteen responses will range from "A very great deal better" to "A very great deal worse".<br><br>(ix) Use of rescue medications to relieve pain at the first metatarsophalangeal joint
The number of <span class="yellow">participants</span> who consumed rescue medication (e.g., paracetamol) and mean consumption of rescue medication to relieve pain at the first MPJ (mean grams of paracetamol/<span class="yellow">participant</span>/month] will be assessed using a medications diary that <span class="yellow">participants</span> will self-complete [38,39]. The diary will be returned to the assessor at monthly intervals for analysis.<br><br>(x) Frequency and severity of adverse events as safety variables
The frequency (number of <span class="yellow">participants</span> affected and number of cases) and types of adverse events (including adverse drug reactions) in each treatment group during the trial will be recorded using a questionnaire that <span class="yellow">participants</span> will complete during the follow-up appointments at 1, 3 and 6 months post-treatment [40]. To classify the 'type' of adverse event, a blinded assessor will classify the adverse event as being serious or non-serious [40]. Any serious adverse events, defined as adverse events leading to serious disability, hospital admission, or prolongation of hospitalisation, life-threatening events; or death) will be further classified using the International Classification of Diseases (ICD) codes [41]. Non-serious adverse events will include both local (pain, effusion and heat, with each classified as mild, moderate, severe) and systemic adverse events. An open-response type format will also be available for <span class="yellow">participant</span> responses.<br><br><br><br>Sample size
The sample size for the study has been pre-specified using an a priori power analysis using the primary outcome measure of the pain domain of the FHSQ [42]. One hundred and forty two <span class="yellow">participants</span> (i.e. 71 per group) will provide power of 90% to detect a minimally important difference in the pain domain of the FHSQ (i.e. 14 points on the FHSQ questionnaire) with the significance level set at p < 0.05. A difference of 14 points was determined to be a clinically significant difference worth detecting [43] and a standard deviation of 25 was derived from a previous report [44]. This calculation included a 5% drop-out rate [13]. However, we will aim to recruit 150 <span class="yellow">participants</span> (~75 <span class="yellow">participants</span> per intervention group). Further, we have conservatively ignored the extra precision provided by covariate analysis when estimating the sample size.<br><br>Statistical analysis
Statistical analysis will be undertaken using SPSS version 14.0 (SPSS Corp, Chicago, Ill, USA) and STATA 8 (Stata Corp, College Station, Tex., USA) statistical software. All analyses will be conducted on an intention-to-treat principle using all randomised <span class="yellow">participants</span> [45-47]. Missing data will be replaced with the last score carried forward [48]. Standard tests for normal distribution will be used and transformation carried out if required.
Demographic characteristics (gender, age, weight, height, body mass index) will be determined for the baseline visit for each treatment group. Summary statistics will be calculated for duration of symptoms, side affected (left, right, bilateral), grade of OA at the first MPJ [19] as well as all primary and secondary outcome measurements for each treatment group.
Analyses will be conducted on 1, 3 and 6 month outcome measures. The continuously-scored outcome measures at 1, 3 and 6 months will be compared using analysis of covariance with baseline scores and intervention group entered as independent variables [49,50]. The exception to this will be the plantar pressure measurements which will be analysed at baseline, 1, 3 and 6 months using two-way repeated measures analysis of variance statistics. Post-hoc comparisons will be performed using Bonferroni-adjusted t-tests. Nominal and ordinal scaled data will be compared at 1, 3 and 6 months using Mann-Whitney U-tests and chi-square analyses (or Fisher's exact test where appropriate) respectively. Effect sizes will be determined using Cohen's d (continuous scaled data) or odds ratios (nominal scaled data and ordinal scaled data) as appropriate.
The outcome measurements obtained at 7 or 9 months for <span class="yellow">participants</span> that receive a second and final intra-articular injection (of Synvisc® or sterile saline according to the treatment group they are in) on days 30 or 90 respectively, will also be analysed as described above. These analyses will be classified as secondary outcomes.<br><br>
Discussion
This study is a randomised placebo controlled trial designed to investigate the efficacy of intra-articular hyaluronan (Synvisc®) to reduce pain and improve function in <span class="yellow">people</span> with OA of the first MPJ (hallux limitus). Two studies have previously investigated the efficacy of intra-articular hyaluronan for the treatment of first MPJ OA [13,14]. However, neither of these studies used a placebo control group. To our knowledge, this is the first randomised controlled trial using intra-articular hyaluronan for OA of the first MPJ.
The use of a placebo control group is essential for studies evaluating the effects of intra-articular therapies as there is likely to be a large placebo response related to the injection procedure and this may inflate the results in uncontrolled evaluations [51]. Indeed, a recent meta-analysis of hyaluronan for knee OA concluded that a placebo effect accounted for 79% of the efficacy of intra-articular hyaluronan [16].
The study protocol and outcome measures have been developed in accordance of the recommendations of the OARSI Clinical Trials Task Force guidelines [18]. The outcome measures are pain and function subscales of the FHSQ, pain and stiffness at the first MPJ, range of motion (dorsiflexion) of the first MPJ, plantar flexion strength of muscles of the first MPJ, generic health related quality of life (SF-36), <span class="yellow">patient</span> satisfaction with treatment, consumption of rescue medication as well as frequency and nature of adverse effects. These outcomes will be measured at baseline then at 1, 3 and 6 months after treatment. Previous research suggests that the effects of intra-articular hyaluronan persist for up to 12 months following treatment [9,38]. Thus, the use of follow-up assessments at 6 month post-treatment will allow us to determine if the effects, if any, of intra-articular hyaluronan persist in the longer term.
<span class="blue">Participants</span> will be given the option of a second and final intra-articular injection (of Synvisc® or sterile saline according to the treatment group they are in) on days 30 or 90 if there is no improvement in their symptoms. Although this has the potential to complicate the interpretation of the results of the study, this protocol was included as it is likely to be more reflective of clinical practice [14], and this is in keeping with the pragmatic nature of this trial.
In summary, this project is the first randomised controlled trial to be conducted to evaluate the efficacy of intra-articular hyaluronan for reducing pain and improving function in <span class="yellow">people</span> with hallux limitus. The study protocol, including interventions, have been pragmatically designed to ensure that the study findings are generaliseable to clinical practice. Recruitment for the study will commence in June 2008, and we expect final results to be available in mid-2010.<br><br>Competing interests
HBM and KBL are Editor-in-Chief and Deputy Editor-in-Chief, respectively, of Journal of Foot and Ankle Research. It is journal policy that editors are removed from the peer review and editorial decision making processes for papers they have co-authored.<br><br>Authors' contributions
SEM, HBM, KBL and CJH conceived the idea and obtained funding for the study. SEM, HBM, KBL, AEZ and JDL designed the trial protocol. SEM, HBM, KBL and GVZ drafted the manuscript. All authors have read and approved the final manuscript.<br><br>
<h3>pmcA1524813</h3>Comparison of age-specific cataract prevalence in two population-based surveys 6 years apart
Abstract
Background
In this study, we aimed to compare age-specific cortical, nuclear and posterior subcapsular (PSC) cataract prevalence in two surveys 6 years apart.<br><br>Methods
The Blue Mountains Eye Study examined 3654 <span class="yellow">participants</span> (82.4% of those eligible) in cross-section I (1992–4) and 3509 <span class="yellow">participants</span> (75.1% of survivors and 85.2% of newly eligible) in cross-section II (1997–2000, 66.5% overlap with cross-section I). Cataract was assessed from lens photographs following the Wisconsin Cataract Grading System. Cortical cataract was defined if cortical opacity comprised ≥ 5% of lens area. Nuclear cataract was defined if nuclear opacity ≥ Wisconsin standard 4. PSC was defined if any present. Any cataract was defined to include <span class="yellow">persons</span> who had previous cataract surgery. Weighted kappa for inter-grader reliability was 0.82, 0.55 and 0.82 for cortical, nuclear and PSC cataract, respectively. We assessed age-specific prevalence using an interval of 5 years, so that <span class="yellow">participants</span> within each age group were independent between the two surveys.<br><br>Results
Age and gender distributions were similar between the two populations. The age-specific prevalence of cortical (23.8% in 1st, 23.7% in 2nd) and PSC cataract (6.3%, 6.0%) was similar. The prevalence of nuclear cataract increased slightly from 18.7% to 23.9%. After age standardization, the similar prevalence of cortical (23.8%, 23.5%) and PSC cataract (6.3%, 5.9%), and the increased prevalence of nuclear cataract (18.7%, 24.2%) remained.<br><br>Conclusion
In two surveys of two population-based samples with similar age and gender distributions, we found a relatively stable cortical and PSC cataract prevalence over a 6-year period. The increased prevalence of nuclear cataract deserves further study.<br><br><br><br>Background
Age-related cataract is the leading cause of reversible visual impairment in older <span class="yellow">persons</span> [1-6]. In Australia, it is estimated that by the year 2021, the number of <span class="yellow">people</span> affected by cataract will increase by 63%, due to population aging [7]. Surgical intervention is an effective treatment for cataract and normal vision (> 20/40) can usually be restored with intraocular lens (IOL) implantation.
Cataract surgery with IOL implantation is currently the most commonly performed, and is, arguably, the most cost effective surgical procedure worldwide. Performance of this surgical procedure has been continuously increasing in the last two decades. Data from the Australian Health Insurance Commission has shown a steady increase in Medicare claims for cataract surgery [8]. A 2.6-fold increase in the total number of cataract procedures from 1985 to 1994 has been documented in Australia [9]. The rate of cataract surgery per thousand <span class="yellow">persons</span> aged 65 years or older has doubled in the last 20 years [8,9]. In the Blue Mountains Eye Study population, we observed a one-third increase in cataract surgery prevalence over a mean 6-year interval, from 6% to nearly 8% in two cross-sectional population-based samples with a similar age range [10]. Further increases in cataract surgery performance would be expected as a result of improved surgical skills and technique, together with extending cataract surgical benefits to a greater number of older <span class="yellow">people</span> and an increased number of <span class="yellow">persons</span> with surgery performed on both eyes.
Both the prevalence and incidence of age-related cataract link directly to the demand for, and the outcome of, cataract surgery and eye health care provision. This report aimed to assess temporal changes in the prevalence of cortical and nuclear cataract and posterior subcapsular cataract (PSC) in two cross-sectional population-based surveys 6 years apart.<br><br>Methods
The Blue Mountains Eye Study (BMES) is a population-based cohort study of common eye diseases and other health outcomes. The study involved eligible permanent residents aged 49 years and older, living in two postcode areas in the Blue Mountains, west of Sydney, Australia. <span class="blue">Participants</span> were identified through a census and were invited to participate. The study was approved at each stage of the data collection by the <span class="yellow">Human</span> Ethics Committees of the University of Sydney and the Western Sydney Area Health Service and adhered to the recommendations of the Declaration of Helsinki. Written informed consent was obtained from each <span class="yellow">participant</span>.
Details of the methods used in this study have been described previously [11]. The baseline examinations (BMES cross-section I) were conducted during 1992–1994 and included 3654 (82.4%) of 4433 eligible residents. Follow-up examinations (BMES IIA) were conducted during 1997–1999, with 2335 (75.0% of BMES cross section I survivors) participating. A repeat census of the same area was performed in 1999 and identified 1378 newly eligible residents who moved into the area or the eligible age group. During 1999–2000, 1174 (85.2%) of this group participated in an extension study (BMES IIB). BMES cross-section II thus includes BMES IIA (66.5%) and BMES IIB (33.5%) <span class="yellow">participants</span> (n = 3509).
Similar procedures were used for all stages of data collection at both surveys. A questionnaire was administered including demographic, family and medical history. A detailed eye examination included subjective refraction, slit-lamp (Topcon SL-7e camera, Topcon Optical Co, Tokyo, Japan) and retroillumination (Neitz CT-R camera, Neitz Instrument Co, Tokyo, Japan) photography of the lens. Grading of lens photographs in the BMES has been previously described [12]. Briefly, masked grading was performed on the lens photographs using the Wisconsin Cataract Grading System [13]. Cortical cataract and PSC were assessed from the retroillumination photographs by estimating the percentage of the circular grid involved. Cortical cataract was defined when cortical opacity involved at least 5% of the total lens area. PSC was defined when opacity comprised at least 1% of the total lens area. Slit-lamp photographs were used to assess nuclear cataract using the Wisconsin standard set of four lens photographs [13]. Nuclear cataract was defined when nuclear opacity was at least as great as the standard 4 photograph. Any cataract was defined to include <span class="yellow">persons</span> who had previous cataract surgery as well as those with any of three cataract types. Inter-grader reliability was high, with weighted kappa 0.82 for cortical cataract, 0.55 (simple kappa 0.75) for nuclear cataract and 0.82 for PSC grading. The intra-grader reliability for nuclear cataract was assessed with simple kappa 0.83 for the senior grader who graded nuclear cataract at both surveys. All PSC cases were confirmed by an ophthalmologist (PM).
In cross-section I, 219 <span class="yellow">persons</span> (6.0%) had missing or ungradable Neitz photographs, leaving 3435 with photographs available for cortical cataract and PSC assessment, while 1153 (31.6%) had randomly missing or ungradable Topcon photographs due to a camera malfunction, leaving 2501 with photographs available for nuclear cataract assessment. Comparison of characteristics between <span class="yellow">participants</span> with and without Neitz or Topcon photographs in cross-section I showed no statistically significant differences between the two groups, as reported previously [12]. In cross-section II, 441 <span class="yellow">persons</span> (12.5%) had missing or ungradable Neitz photographs, leaving 3068 for cortical cataract and PSC assessment, and 648 (18.5%) had missing or ungradable Topcon photographs, leaving 2860 for nuclear cataract assessment.
Data analysis was performed using the Statistical Analysis System (SAS, SAS Institute, Cary, NC, USA). Age-adjusted prevalence was calculated using direct standardization of the cross-section II population to the cross-section I population. We assessed age-specific prevalence using an interval of 5 years, so that <span class="yellow">participants</span> within each age group were independent between the two cross-sectional surveys.<br><br>Results
Characteristics of the two survey populations have been previously compared [14] and showed that age and sex distributions were similar. Table 1 compares <span class="yellow">participant</span> characteristics between the two cross-sections. Cross-section II <span class="yellow">participants</span> generally had higher rates of diabetes, hypertension, myopia and more users of inhaled steroids.
Cataract prevalence rates in cross-sections I and II are shown in Figure 1. The overall prevalence of cortical cataract was 23.8% and 23.7% in cross-sections I and II, respectively (age-sex adjusted P = 0.81). Corresponding prevalence of PSC was 6.3% and 6.0% for the two cross-sections (age-sex adjusted P = 0.60). There was an increased prevalence of nuclear cataract, from 18.7% in cross-section I to 23.9% in cross-section II over the 6-year period (age-sex adjusted P < 0.001). Prevalence of any cataract (including <span class="yellow">persons</span> who had cataract surgery), however, was relatively stable (46.9% and 46.8% in cross-sections I and II, respectively).
After age-standardization, these prevalence rates remained stable for cortical cataract (23.8% and 23.5% in the two surveys) and PSC (6.3% and 5.9%). The slightly increased prevalence of nuclear cataract (from 18.7% to 24.2%) was not altered.
Table 2 shows the age-specific prevalence rates for cortical cataract, PSC and nuclear cataract in cross-sections I and II. A similar trend of increasing cataract prevalence with increasing age was evident for all three types of cataract in both surveys. Comparing the age-specific prevalence between the two surveys, a reduction in PSC prevalence in cross-section II was observed in the older age groups (≥ 75 years). In contrast, increased nuclear cataract prevalence in cross-section II was observed in the older age groups (≥ 70 years). Age-specific cortical cataract prevalence was relatively consistent between the two surveys, except for a reduction in prevalence observed in the 80–84 age group and an increasing prevalence in the older age groups (≥ 85 years).
Similar gender differences in cataract prevalence were observed in both surveys (Table 3). Higher prevalence of cortical and nuclear cataract in <span class="yellow">women</span> than <span class="yellow">men</span> was evident but the difference was only significant for cortical cataract (age-adjusted odds ratio, OR, for <span class="yellow">women</span> 1.3, 95% confidence intervals, CI, 1.1–1.5 in cross-section I and OR 1.4, 95% CI 1.1–1.6 in cross-section II). In contrast, <span class="yellow">men</span> had slightly higher PSC prevalence than <span class="yellow">women</span> in both cross-sections but the difference was not significant (OR 1.1, 95% CI 0.8–1.4 for <span class="yellow">men</span> in cross-section I and OR 1.2, 95% 0.9–1.6 in cross-section II).<br><br>Discussion
Findings from two surveys of BMES cross-sectional populations with similar age and gender distribution showed that the prevalence of cortical cataract and PSC remained stable, while the prevalence of nuclear cataract appeared to have increased. Comparison of age-specific prevalence, with totally independent samples within each age group, confirmed the robustness of our findings from the two survey samples. Although lens photographs taken from the two surveys were graded for nuclear cataract by the same graders, who documented a high inter- and intra-grader reliability, we cannot exclude the possibility that variations in photography, performed by different photographers, may have contributed to the observed difference in nuclear cataract prevalence. However, the overall prevalence of any cataract (including cataract surgery) was relatively stable over the 6-year period.
Although different population-based studies used different grading systems to assess cataract [15], the overall prevalence of the three cataract types were similar across different study populations [12,16-23]. Most studies have suggested that nuclear cataract is the most prevalent type of cataract, followed by cortical cataract [16-20]. Ours and other studies reported that cortical cataract was the most prevalent type [12,21-23].
Our age-specific prevalence data show a reduction of 15.9% in cortical cataract prevalence for the 80–84 year age group, concordant with an increase in cataract surgery prevalence by 9% in those aged 80+ years observed in the same study population [10]. Although cortical cataract is thought to be the least likely cataract type leading to a cataract surgery, this may not be the case in all older <span class="yellow">persons</span>.
A relatively stable cortical cataract and PSC prevalence over the 6-year period is expected. We cannot offer a definitive explanation for the increase in nuclear cataract prevalence. A possible explanation could be that a moderate level of nuclear cataract causes less visual disturbance than the other two types of cataract, thus for the oldest age groups, <span class="yellow">persons</span> with nuclear cataract could have been less likely to have surgery unless it is very dense or co-existing with cortical cataract or PSC. Previous studies have shown that functional vision and reading performance were high in <span class="yellow">patients</span> undergoing cataract surgery who had nuclear cataract only compared to those with mixed type of cataract (nuclear and cortical) or PSC [24,25]. In addition, the overall prevalence of any cataract (including cataract surgery) was similar in the two cross-sections, which appears to support our speculation that in the oldest age group, nuclear cataract may have been less likely to be operated than the other two types of cataract. This could have resulted in an increased nuclear cataract prevalence (due to less being operated), compensated by the decreased prevalence of cortical cataract and PSC (due to these being more likely to be operated), leading to stable overall prevalence of any cataract.
Possible selection bias arising from selective survival among <span class="yellow">persons</span> without cataract could have led to underestimation of cataract prevalence in both surveys. We assume that such an underestimation occurred equally in both surveys, and thus should not have influenced our assessment of temporal changes.
Measurement error could also have partially contributed to the observed difference in nuclear cataract prevalence. Assessment of nuclear cataract from photographs is a potentially subjective process that can be influenced by variations in photography (light exposure, focus and the slit-lamp angle when the photograph was taken) and grading. Although we used the same Topcon slit-lamp camera and the same two graders who graded photos from both surveys, we are still not able to exclude the possibility of a partial influence from photographic variation on this result.
A similar gender difference (<span class="yellow">women</span> having a higher rate than <span class="yellow">men</span>) in cortical cataract prevalence was observed in both surveys. Our findings are in keeping with observations from the Beaver Dam Eye Study [18], the Barbados Eye Study [22] and the Lens Opacities Case-Control Group [26]. It has been suggested that the difference could be related to hormonal factors [18,22]. A previous study on biochemical factors and cataract showed that a lower level of iron was associated with an increased risk of cortical cataract [27]. No interaction between sex and biochemical factors were detected and no gender difference was assessed in this study [27]. The gender difference seen in cortical cataract could be related to relatively low iron levels and low hemoglobin concentration usually seen in <span class="yellow">women</span> [28]. Diabetes is a known risk factor for cortical cataract but in this particular population diabetes is more prevalent in <span class="yellow">men</span> than <span class="yellow">women</span> in all age groups [29]. Differential exposures to cataract risk factors or different dietary or lifestyle patterns between <span class="yellow">men</span> and <span class="yellow">women</span> may also be related to these observations and warrant further study.<br><br>Conclusion
In summary, in two population-based surveys 6 years apart, we have documented a relatively stable prevalence of cortical cataract and PSC over the period. The observed overall increased nuclear cataract prevalence by 5% over a 6-year period needs confirmation by future studies, and reasons for such an increase deserve further study.<br><br>Competing interests
The author(s) declare that they have no competing interests.<br><br>Authors' contributions
AGT graded the photographs, performed literature search and wrote the first draft of the manuscript. JJW graded the photographs, critically reviewed and modified the manuscript. ER performed the statistical analysis and critically reviewed the manuscript. PM designed and directed the study, adjudicated cataract cases and critically reviewed and modified the manuscript. All authors read and approved the final manuscript.<br><br>Pre-publication history
The pre-publication history for this paper can be accessed here:<br><br><br><br><h3>pmcA1590010</h3>Drug information resources used by nurse practitioners and collaborating physicians at the point of care in Nova Scotia, Canada: a survey and review of the literature
Abstract
Background
Keeping current with drug therapy information is challenging for health care practitioners. Technologies are often implemented to facilitate access to current and credible drug information sources. In the Canadian province of Nova Scotia, legislation was passed in 2002 to allow nurse practitioners (NPs) to practice collaboratively with physician partners. The purpose of this study was to determine the current utilization patterns of information technologies by these groups of practitioners.<br><br>Methods
Nurse practitioners and their collaborating physician partners in Nova Scotia were sent a survey in February 2005 to determine the frequency of use, usefulness, accessibility, credibility, and current/timeliness of personal digital assistant (PDA), computer, and print drug information resources. Two surveys were developed (one for PDA users and one for computer users) and revised based on a literature search, stakeholder consultation, and pilot-testing results. A second distribution to nonresponders occurred two weeks following the first. Data were entered and analysed with SPSS.<br><br>Results
Twenty-seven (14 NPs and 13 physicians) of 36 (75%) recipients responded. 22% (6) returned personal digital assistant (PDA) surveys. Respondents reported print, health professionals, and online/electronic resources as the most to least preferred means to access drug information, respectively. 37% and 35% of respondents reported using "both print and electronic but print more than electronic" and "print only", respectively, to search monograph-related drug information queries whereas 4% reported using "PDA only". Analysis of respondent ratings for all resources in the categories print,  health professionals and other, and online/electronic resources, indicated  that the Compendium of Pharmaceuticals and Specialties and pharmacists  ranked highly for frequency of use, usefulness, accessibility, credibility,  and current/timeliness by both groups of practitioners. Respondents' preferences and resource ratings were consistent with self-reported methods for conducting drug information queries. Few differences existed between NP and physician rankings of resources.<br><br>Conclusion
The use of computers and PDAs remains limited, which is also consistent with preferred and frequent use of print resources. Education for these practitioners regarding available electronic drug information resources may facilitate future computer and PDA use. Further research is needed to determine methods to increase computer and PDA use and whether these technologies affect prescribing and <span class="yellow">patient</span> outcomes.<br><br><br><br>Background
Challenges with knowledge management for health care professionals
In 1986, Haynes et al. published a series of 6 articles entitled "how to keep up with the medical literature" in an effort to help clinicians with information management, but this challenge has not decreased in last two decades [1-6]. Alper et al. suggest that maintaining currency with relevant literature in primary care would "require 627.5 hours per month, or about 29 hours per weekday, or 3.6 full-time equivalents of physician effort" [7]. The volume of information associated with keeping up to date is frequently cited as a barrier [8]. It is estimated that annually there are approximately 10,000 new randomized trials in MEDLINE and over 450,000 clinical trials identified by the Cochrane Collaboration [9,10]. Keeping up to date has been described with several analogies including clinicians attempting to drink water from a fire hose and swimming in rivers of clinical research with unprecedented depth, velocity, and turbulence [11,12].
Difficulties with dissemination of research evidence and keeping up to date on pharmacotherapeutic interventions are reported despite the development of tools such as clinical practice guidelines and systematic reviews that are intended to reduce the need for practitioners to evaluate original research [13]. To complicate matters further, there are often issues of credibility, timeliness, and volume of clinical practice guidelines and reviews. Many guidelines are criticized for their methodological development. Shaneyfelt et al. reviewed 279 guidelines for methodological standards from peer reviewed medical literature [14]. These authors found that only 51%, 33.6%, and 46% adhered to standards on guideline development and format, evidence identification and summary, and formulation of recommendations, respectively [14]. A Canadian review on drug therapy guidelines found significant variation in quality depending on the developer [13]. Approximately 25% of guidelines were not recommended for use in practice by the appraisers' criteria [13]. As an example of the volume of clinical practice guidelines available, eleven recent guidelines on community acquired pneumonia exist [15]. To add to the complexities involved with keeping current with pharmacotherapeutic management strategies, as of 2000, there were over 22,000 drug products approved for sale in Canada for <span class="yellow">human</span> use [16].
There is also considerable debate regarding what constitutes "evidence" in practice, which contributes to confusion for clinicians [17,18]. Sim et al. succinctly describe the gap between evidence and action as difficulties with obtaining, systematically reviewing, applying in context, and measuring the outcome following application of evidence [19].<br><br>Maintaining competence – nurse practitioners as a new group of prescribers
Competencies for nurse practitioners (NPs) on a local and international level include critically appraising and applying literature and research findings in practice [20-23]. The Canadian Nurses Association (CNA) has developed the Canadian Nurse Practitioner Core Competency Framework that describes the knowledge, skills, judgment, and attributes required for practice. Evidence based practice is integral to pharmacotherapeutic interventions and prescribing competencies [23]. The National Prescribing Centre, an organization of the National Health Service in the UK, describes several competencies around information needs relevant to prescribing and emphasis is placed on using relevant and up to date information in various formats (e.g. print, electronic, verbal). Several related competencies include understanding advantages and disadvantages of information sources and the currency of resources [21]. Researchers in the US developed NP informatics competencies for integration into advanced nursing practice curricula [24]. Competencies related to informatics knowledge include critical analysis of data and information for use in evidence based practice, evaluating and applying relevant information, synthesizing best evidence, and using optimal search strategies to locate clinically sound and useful studies from information resources [24]. Achieving and maintaining competence in these domains as well as a solid foundation in pharmacology is necessary to support NPs in their relatively new role as a prescriber [25-27].<br><br>Knowledge management and information seeking behaviours among nurse practitioners and physicians
Information seeking behaviours of physicians are better documented than NPs [11]. Information related to diagnosis is important to both groups but drug therapy queries may occur more frequently with NPs [28-33]. Research on nurses' behaviours related to information seeking is available from the hospital setting [33-35] but the generalizability of these behaviours to NPs with a prescribing role is unclear. Differences in nursing roles, responsibilities, and legislation, including prescriptive authority, exist depending on the country of practice.<br><br>Nurse practitioners and their collaborating physician partners in Nova Scotia
Nova Scotia is a Canadian province with a population of approximately 942,000 [36]. The province is divided into six health zones that include nine district health authorities, one of which includes the provincial capital and is considered to be urban [37,38]. Health care service delivery is challenging due to many factors including the rural nature of the province, which is estimated to be 60% of population [37,39].
Starting in 1998, the Nova Scotia Department of Health led an initiative to explore different methods of delivering, managing, and funding primary care services. The Strengthening Primary Care in Nova Scotia Communities Initiative (SPCI) was established with the selection of four primary care demonstration sites where a primary health care NP was hired to practice collaboratively with one or more family/general physicians and other members of an interdisciplinary team. Each demonstration site adopted alternative (non fee-for-service) physician payment mechanisms and used electronic <span class="yellow">patient</span> records (EPRs) to support service delivery [41]. Demonstration sites participated in project evaluation components that included, but were not limited to, NP roles, alternative fee structures, consumer satisfaction, and implementation and integration of EPRs [41,42].
Legislation to allow NPs to practice collaboratively with physicians in Nova Scotia was passed in 2002, part way through the SPCI project [39]. Prescriptive authority granted through legislation authorizes NPs to prescribe from a schedule of drugs [43,44]. At the time of conducting this research project, 16 primary health care NPs were in active practice [43].
The EPR component of the SPCI project evaluation provided information on the use of technologies in the community context. Results from the implementation process indicated that considerable attention is required for technology literacy, time for training, and selection of software for EPRs [41]. Although the majority of community-based, non-institutional clinical practice settings in Nova Scotia primarily operate with paper-based charting systems, there is a movement toward integrating electronic technologies, including the EPR, in practice among health care providers, administrators, and the provincial government. In addition to recording <span class="yellow">patient</span> visit information, a component of the EPR package serves to provide drug information resources.
Drug therapy information resources for NPs and nurse prescribers have frequently been described as essential in supporting practice [25,28,29]. The role of NPs is relatively new in Canada [39] and there is limited information available to indicate the type of resources (e.g. print, electronic, EPR based) these prescribers use for drug and therapeutic information queries at the point of care. It is unknown as to whether differences exist regarding types of resources used, drug information needs, and utilization patterns among NPs and collaborating physician partners. Some research has suggested that the degree of multidisciplinary team functioning relates to the adoption of technology or innovations in practice but more research is required to determine the extent of these relationships [45,46].
The use of EPR technology is increasing in Nova Scotia but little information is available regarding the readiness of practitioners for use of specific features such as drug information resources. Based on the EPR related results of the SPCI evaluation, use of these functions could be challenging without proper facilitation. The purpose of the survey for this research was to describe drug information resources used by NPs and their collaborating physician partners at the point of care. The results of the survey will be used to guide further technology implementation strategies and stimulate further discussion around drug information resource usage at the point of care.<br><br>
Methods
Survey development
Survey development involved three stages including identification of important content areas, development of draft questions, and survey refinement.
Identifying important content areas for inclusion in the survey involved conducting a comprehensive English language literature search, consultation with relevant stakeholders (e.g. members of the Nova Scotia Department of Health), and input from subject matter experts at Dalhousie University. The literature review was conducted using the following bibliographic databases: PubMed, Cumulative Index to Nursing and Allied Health Literature (CINAHL), International Pharmaceutical Abstracts (IPA), and Web of Science Citation Databases. Hand and electronic searching of relevant journals was also conducted. Broad search terms were used without limits on publication date or place as nurse practitioner titles, roles and scopes of practice, and terminology regarding technology vary nationally and internationally. Some examples of terms used included nurse practitioner, nurse prescriber, nurse clinicians, district nurse, health visitor, drug information resources, drug information services, information needs, and information technology.
The draft survey was reviewed by the research team to reduce the number of items and improve clarity. The layout of the questionnaire was carefully examined to ensure that it was easy to follow and complete. Research results from a previous investigation of Nova Scotian physicians' behaviours regarding drug information were also used to further revise the survey [47]. This draft questionnaire was pilot tested by two out of province NPs and one physician. The results of the pilot were used to make final revisions to the survey. Based on pilot-testing feedback and investigator consensus, the final survey was divided into 2 versions, one for personal digital assistant (PDA) users and one for computer users.
The 10 page surveys for PDA and computer users had 5 or 6 sections, respectively, and 37 questions, many with multiple parts. The survey content included demographics, computer or PDA use and experience, drug and therapeutic resource use and preferences, PDA future use, perceived barriers and facilitators to PDA use, and technology training preferences.
Section one contained demographic questions such as gender, age, job title, volume of <span class="yellow">patients</span>, and EPR availability in the practice setting. Section two was designed to determine PDA or computer use and experience in the practice setting with questions regarding length of use, costs, and work versus home usage. This section also addressed usage and rating of different drug information resources. Resource ratings were based on the frequency of usage, usefulness, accessibility, credibility, and current/timeliness. Resources were grouped as print (i.e. books, journals, and clinical practice guidelines), online/electronic resources, and health professionals and other. Respondents used 5-point Likert scales (strongly agree to strongly disagree) for rating opinions related to resources. A rating of 6 (not applicable, I do not use this resource) was also included for respondents who did not use a particular resource. Frequency of searching for specific information was rated on a 3-point Likert scale (frequently to never). The final sections of the survey included categorical, open-ended, and Likert scale questions regarding preferred resources, technology barriers, PDA future use, and technology training preferences. Copies of the surveys are attached as an appendix in PDF format [see additional file 1 and 2] or can also be accessed from the Initiative for Medication Management, Policy Analysis, Research & Training (IMPART) website [48].
Ethics approval for the survey was granted through Dalhousie University Research Ethics Board on February 3, 2005.<br><br>Survey population
Licensed, actively practicing, primary health care NPs (n = 16) and their collaborating physician partners (n = 21) were eligible to participate.<br><br>Survey procedures
The survey recruitment procedures were based on the methods of Dillman [49] and Salant and Dillman [50]. Survey packages contained a cover letter, separate surveys for PDA and computer users, and a return self-addressed stamped envelope. The covering letter instructed respondents to self-select the appropriate survey (either PDA or computer) based on their drug information seeking behaviours. <span class="blue">Participants</span> who had used a PDA at any time were instructed to complete the PDA version of the survey. Those who had never used a PDA for drug information were instructed to complete the computer version of the survey. Several strategies were used to optimize response rate and included: personalized cover letters, coloured paper for surveys, stamped return envelopes, follow-up mailing, and a priority post mailing [51]. The covering letter included coloured logos of Dalhousie University and the Nova Scotia Department of Health representing the investigator affiliations and endorsement of the project.
A master mailing list with names and addresses of NPs and their collaborating physician partners was created. To maintain confidentiality of respondents, a number placed on the bottom right corner of each survey corresponded to a name on the confidential master mailing sheet. The postage paid return envelopes were addressed to the research coordinator at the School of Nursing, Dalhousie University, who matched respondents to the mailing list from the first distribution. The cross-referenced mailing list was not accessible to those entering or analysing data. The research coordinator sent the second distribution to those who had not initially responded. A fluorescent coloured page was included in the second mailing to notify recipients of the second and final mailing status. The second mailing followed 2 weeks after the initial mailing (February 2005). The surveys were sent via Xpresspost™ through Canada Post.<br><br>Data analyses
Quantitative
Data were entered and analysed in Statistical Package for Social Sciences (SPSS) (version 11.5 for Windows). Five surveys were randomly selected as a check for accuracy of data entry. Descriptive statistics were used to describe resource usage by practitioners. Chi Square (Fisher's Exact when cell count less than 5) analyses were used to determine differences in computer or PDA use based on predetermined variables (e.g. high speed Internet connection, number of <span class="yellow">patients</span> per day). Mann Whitney U tests were used to compare physician and NPs Likert scale ratings (1 = strongly agree to 5 = strongly disagree) of resource use. Physician and NP rankings of all resources (print, online/electronic, and health professionals and other) were determined from means of Likert scale ratings (1 = strongly agree, 5 = strongly disagree) for each of the pre-specified characteristics (e.g. frequency of use, accessibility, etc.) and the frequency of use of the resources. The best rankings were assigned for the lowest mean scores and the largest number of the sample using a resource. These rankings (ranks based on mean and ranks based on sample) were then entered into a formula to calculate an overall rank. The formula includes: rank = [(rank according to % of sample using the resource + rank based on mean score) ÷ 2]. This formula was used to account for mean scores based on small samples as these numbers could potentially over or underestimate the value of a resource. Ratings of 6 (i.e. not applicable, I do not use this resource) were excluded from the analyses.<br><br>Qualitative
Comments were entered in a word-processing program and organized by type of respondent (PDA versus computer) and question number. The coded survey number and respondent type (NP or physician) were also included next to comments. Investigators determined themes and categorized comments based on previous experience, knowledge, and familiarity with the topic.<br><br><br><br>Results
Surveys were completed and returned by 75% of eligible <span class="yellow">participants</span> (27 of 36). One physician survey was undeliverable. The response rates from within the NP and physician samples were 88% and 65%, respectively. Complete demographic information is available in Table 1.
Methods for accessing resources and self-reported resource use
Resource use was similar amongst practitioners. Respondents indicated that print resources (mean 4.56, SD 0.80), health professionals (mean 3.26, SD 0.90), and online/electronic resources (mean 2.70, SD 1.20) were the preferred method (1 = least preferred to 5 = most preferred) for accessing drug information. Thirty-seven percent of respondents reported that searching for specific questions related to drug information (e.g. usual dosage, duration of therapy) was conducted using both print and electronic resources (but print use greater than electronic) (Table 2). The preferred means (i.e. print) to access resources was consistent with the most common means of conducting searches for specific drug information queries.
Respondents' ratings for pre-specified print, online/electronic, and professional resources and other, based on means from Likert scales and number of respondents using the resources, are presented in Tables 3, 4, and 5. Of all resources within the print, online/electronic, and health professionals or other categories, NPs and physicians rated the Compendium of Pharmaceuticals and Specialties (CPS) [52] and pharmacists as the top two most frequently used resources for providing drug and therapeutic information. Physicians rated other physicians as the third most frequently used resource. The book Therapeutic Choices [53] ranked third for NPs. Based on written feedback, physicians and NPs consulted pharmacists and other physicians most frequently. The CPS and pharmacists were also ranked as the top two resources overall in terms of usefulness, accessibility, credibility, and current/timeliness for physicians. Rankings by NPs were similar for usefulness, accessibility, and credibility. NPs ranked pharmacists, Therapeutic Choices, and academic detailing first and the CPS as second for current/timeliness.
Within the online/electronic category, electronic clinical practice guidelines (eCPGs) were rated the highest for all characteristics (e.g. usefulness, credibility). Although eCPGs were highly ranked, approximately 30% of the sample reported not using this resource. Other resources in this category were infrequently used based on respondents' self-reports.
Pharmaceutical industry representatives were used as a source of drug information by 85% and 86% of physicians and NPs, respectively (Table 5). This was higher than regional drug information services (used by 23% of physicians and 50% of NPs). After exclusion of traditional health professionals (i.e. physicians, nurses, pharmacists, allied health) in the health professionals and other category, pharmaceutical industry representatives received rankings for second or third for frequency of use, usefulness, accessibility, credibility, and current/timeliness, based on means and number of respondents using this resource (data not shown).<br><br>Differences between nurse practitioners and physicians
A series of Mann Whitney U tests were used to compare the responses of NPs and physicians on their use of print, online/electronic, and health professional resources. In total 95 statistical tests were conducted. The large number of tests increases the likelihood of a type I error as five significant differences would be expected by chance alone at an alpha threshold of 0.05. It is therefore important to treat these results with caution. A limited number of statistically significant (p < 0.05) differences were identified between physicians and NPs and are reported in Table 6. Therapeutic Choices differed significantly for frequency of use with more NPs making use of this resource. Allied health professionals significantly differed between NPs and physicians for accessibility and current/timeliness while NPs were more in agreement with these characteristics of the resource. Nurse colleague credibility and current/timeliness was rated significantly higher by NPs versus physicians.<br><br>Factors influencing electronic technology use at the point of care
Factors such as gender, age, practitioner type (NP vs physician), accessibility, technical support, Internet connection speed, <span class="yellow">patient</span> volume, presence of an EPR, and home computer use were examined to determine if they were associated with the use of a work computer to search for drug information at the point of care. No statistically significant associations were found (Fisher's Exact).<br><br>Additional resources from respondent comments
Respondents indicated other resources and programs, such as clinical calculators, that they would like to access from their computer or PDA. The top three resources that were desired included Canadian clinical practice guidelines, <span class="yellow">patient</span> education information, and ability to track clinical activities/statistics. Further comments from two NP computer survey respondents revealed that a resource on drug interactions and dosages would be desired. One other NP also indicated "up to date info [sic] on drugs to treat various illnesses ie doseage [sic], length of use etc."<br><br>Computer or personal digital assistant use in practice
Approximately 50% of computer survey respondents reported using their work computers for searching drug or therapeutic information related to <span class="yellow">patient</span> care. Of those respondents, just over half (54%) also reported using their home computer for this purpose. Sixty-seven and 17% of PDA survey respondents reported using their PDA for searching drug or therapeutic information related to <span class="yellow">patient</span> care at work and home, respectively.<br><br>Searching on a weekly basis for specific information related to drugs
Of the 24 specified categories of drug information included in the survey, the majority were reported as infrequently searched and a smaller percentage as never searched by respondents (data not shown). The top three categories rated as frequently searched were side effects, adult or usual drug dosage, and most appropriate drug for an indication. (Table 7)<br><br>Issues related to personal digital assistants
Respondents reported their level of agreement with statements related to how PDAs may influence their practice. The statements included aspects of workload (organization and paper work), convenience, and improving quality of care and <span class="yellow">patient</span> outcomes. (Table 8) Respondents agreed that PDAs are a convenient resource but indicated that PDAs would not decrease paperwork or improve <span class="yellow">patient</span> health outcomes.<br><br>Barriers and facilitators to personal digital assistants: themes from written comments
Peer support from colleagues, convenience, standardized usage, and financial and technical support were the main perceived facilitators to PDA use reported by respondents. The main perceived barrier to PDA use reported by respondents (n = 10) included cost. Other factors such as technology literacy, time, lack of peer support, no high speed internet for downloads, lack of needed resources, keeping up to date on resources, and searching speed were also reported.<br><br>Future use of personal digital assistants
Fifty-two percent, including current PDA users, reported that they would use a PDA in the future. Twenty two percent were  uncertain and 19% reported that they would not use a PDA in the future. Two <span class="yellow">people</span> did not respond.<br><br>Confidentiality
Fifty two percent of respondents indicated that <span class="yellow">patient</span> confidentiality with PDAs was no more concerning compared to use of other technologies. Forty-four percent did not know if they had a policy on <span class="yellow">patient</span> confidentiality with regard to technologies.<br><br>Technology training and reimbursement
Respondents rated (1 = least preferred to 5 = most preferred) one on one instruction and group learning led by an expert facilitator as the most preferred (mean 4.32, SD 0.99) means by which to receive instruction on a new technology. Least preferred methods included online discussions/chatrooms (mean 1.52, SD 1.04), internet videos (live: mean 1.70, SD 1.10, or static: mean 1.87, SD 1.14), video cassettes (mean 2.30, SD 1.55), trial and error learning (mean 2.32, SD 1.28), and written manuals (mean 2.92, SD 1.44). Paid leave for attendance at technology training sessions was the preferred means (mean 1.77, SD 0.86; 1 = strongly agree to 5 = strongly disagree) of remuneration for respondents. Respondents also indicated that if financial remuneration was to occur, it should correspond to the amount of time for training that is required (versus a flat rate) (mean 1.96, SD 1.08). Continuing education credits were not viewed as an incentive (mean 2.69, SD 1.44).<br><br>
Discussion
Preferred resources
In our study, printed materials (e.g. compendia, journals, textbook resources) and professionals (e.g. pharmacists) were the most preferred and frequently used means to access information. Physician reliance on text and compendia relative to online/electronic resources has been frequently reported [11]. In a study examining family doctors' use of information sources to answer clinical questions, <span class="yellow">human</span> resources (e.g. doctor, pharmacist), non-prescribing print information (e.g. textbooks and journal articles), and prescribing texts were used 36%, 32%, and 25% of the time, respectively [54]. Books from the workplace were reported by approximately 79% of UK primary care nurses as a commonly used source of knowledge and information used to support practice [55]. Fewer than one-third (31%) reported using electronic resources (e.g. Internet, electronic journals) for this purpose [55]. Results of a postal questionnaire to NPs demonstrated that 61% and 51% of respondents reported using drug reference manuals and textbooks, respectively, a few times a week or more [29]. These frequencies were second and third only to consulting with their physician supervisor (63%). Data from structured interviews of a sample of 22 community nurse prescribers reported by Hall et al. revealed that the majority relied on print materials to access information, namely the British National Formulary [32]. A survey of a primary care practice-based research network in the US that included physicians, physician assistants, and nurse practitioners, revealed that interpersonal and rapidly accessed print resources were preferred. Sixty-one and 58% of respondents reported using drug reference sources such as the Physician's Desk Reference (PDR) and medical textbooks, respectively, a few times a day or daily [56].
The clinicians in our sample perceived the Canadian compendium, the CPS, to be useful, accessible, credible, and current/timely. The CPS, is described as "the Canadian drug reference for health professionals" and is intended to provide a central source of drug information on drug products available in Canada [52]. It is available in print (English and French) and became available online in June 2004. The CPS includes drug monographs for commonly used products approved for use in Canada, but it does not include all drugs available on the Canadian market [57]. The majority of these product monographs are based on monographs submitted by pharmaceutical manufacturers and approved by Health Canada. Some of the monographs are written by the Canadian Pharmacists Association and are described as being evidence-based [52]. The CPS also includes more than 100 pages of clinical tools [52]. The CPS has been criticized for including pharmaceutical company advertising and requiring manufacturer payment for inclusion of product monographs [58]. The accuracy of particular components of CPS monographs has also been investigated. A review of overdose management in 119 monographs from the 2001 CPS revealed considerable variability in the utility of information with 50% of the monographs containing misleading or dangerous advice [59]. Since 2004, the CPS has included an alert box in the overdose section of monographs notifying users to contact Poison Control Centres for overdose management information. Some authors have criticized references that are similar to the CPS as being inadequate with regard to inclusion of evidence based information [60].
The NPs in our sample also rated Therapeutic Choices highly for all characteristics.  This finding is most likely  attributable to the fact that it is a recommended resource for coursework  associated with the Dalhousie NP university program curriculum.  Therapeutic Choices is a concise therapeutics reference text published by the Canadian Pharmacists Association. The text contains approximately 120 extensively referenced chapters with a disease management approach including easy to use algorithms and tables. An editorial board is responsible for extensively reviewing the content to ensure unbiased and objective information is presented [53].<br><br>Health professionals
Reliance on other health professionals, especially pharmacists and physicians, as a resource for information was evident from our study and concurs with the findings of others [28,32,55,61]. Nurse practitioners have reported that collaborative relationships with pharmacists increase NP role satisfaction [61]. NPs frequently consult with allied health care professionals in their primary health care provider role and this is supported by written feedback from our sample regarding frequently consulted health professionals. Nursing colleagues are also likely to be rated highly by NPs due to their affiliation with peers from the same profession.
Some investigators have shown that non-<span class="yellow">human</span> references (e.g. textbook) are sought for more technical aspects of prescribing (e.g. dose), whereas guidance regarding selection of agents (i.e. right drug for an indication) is sought from <span class="yellow">human</span> resources (e.g. pharmacists or physicians) [62]. We were unable to determine what kinds of resources were used for specific purposes from our study.<br><br>Online and electronic resources, computers, and personal digital assistants
From our study, computer survey respondents ranked online/electronic resources third in preference following print and health professionals. Various barriers and facilitators to accessing information online/electronically or via the Internet have been described in the literature [55,63-66]. Variables that have been described by others as barriers such as accessibility, high speed internet access, <span class="yellow">patient</span> volume, age, practitioner type, and technology support did not appear to influence computer searching for information on drugs or therapeutics related to <span class="yellow">patient</span> care in our results. Some qualitative feedback does however support this notion. As an example, in response to a request for a rationale for not using computers one physician commented: "Retro tech [sic]/old fashion. I still like to use my mind and have always been a fan of pen and paper". Barriers that were identified with our sample regarding the use of handheld technologies such as PDAs included cost, time, and issues related to technology literacy. Several <span class="yellow">people</span> questioned the value of PDAs. One GP stated when referring to a PDA: "So far I have not discovered a use for one". Other respondents reinforced their preferences for other resources (e.g. books) and resistance to technology. When responding to barriers for the use of PDAs, one NP commented, "My huge dislike for machinery that frequently requires updating and patience". A physician responded, "as stated, I like to use my own mind, and can get all the info I need from books relatively quickly". Facilitators to the use of PDAs mainly included convenience factors such as having resources all in one place, faster means to get information, and portability. Our sample was not in agreement with some convenience factors in that they did not feel that PDAs would decrease paperwork. Practitioners from our sample felt relatively neutral about PDAs improving <span class="yellow">patient</span>'s health outcomes with 41% responding in this manner. Results from a sample of primary care practitioners in the US revealed that 76% agreed that the use of handheld devices for electronic prescribing would substantially reduce medical errors and improve the quality of health care [67].
Our study also suggests that resources such as the Cochrane Library and its Database of Systematic Reviews were not frequently used. This finding is similar to that of other investigators [30,35,64]. Despite the desire of some clinicians to use these resources, lack of confidence and ability to use them appropriately has been found [30,64,68,69]. Our study suggests that although this resource is perceived as credible, current/timely, and useful, it is also perceived to be somewhat inaccessible. The Cochrane Library is available to the health professionals (e.g. nurses, physicians, pharmacists, occupational therapists, physiotherapists, etc.) in our sample through professional bodies via the Atlantic Health Knowledge Partnership [70].<br><br>Technology training: preferences and incentives
With regard to receiving training for a new technology, our study demonstrates that in <span class="yellow">person</span> conferences or one on one training sessions are the preferred means to receive continuing education. <span class="yellow">Person</span> to <span class="yellow">person</span> interaction has been reported as the preferred and most frequently used means to access continuing education or training by other investigators [55,71].
Our study also indicates that this group of practitioners may benefit from accessing resources [72-80] that provide guidance on useful drug information resources available for devices such as PDAs. This is exemplified by one respondent's statement "knowledge regarding good software programs" as a barrier to the use of PDAs.<br><br>Pharmaceutical industry
The influence of the pharmaceutical industry on physician prescribing and research outcomes has been documented [81,82]. Although NP use of industry representatives as a source of pharmacological information has been documented, the influence on prescribing is largely uninvestigated [32,61,83-85]. The CNA competency framework includes a statement regarding prescribing and industry relations [23]. In our study, the physician and NP rankings of industry representatives were similar. Within the health professionals and other category, pharmaceutical representatives were used as a resource by more of the sample than regional drug information services and comparably to academic detailing services. Academic detailing is a form of continuing  medical education where a trained health professional visits prescribers for  a fifteen to twenty minute session to provide objective information  regarding a therapeutic topic based on best available evidence [86,87]. Following academic detailing, physician and NP rankings of pharmaceutical industry representatives were second or third for frequency of use, usefulness, accessibility, credibility, and current/timeliness.<br><br>Limitations
We do not have demographics or information regarding the reasons why survey recipients did not respond. As per ethical requirements to maintain confidentiality of respondents, we were not able to match respondents from their respective place of practice and therefore cannot conclude whether the practitioners within a practice setting influenced the others' responses. The sample size of the survey is small although it includes 88% response from community based NPs in Nova Scotia. The generalizability of the results is limited due to the variations in NP scopes of practice nationally and internationally. It is unknown whether the findings are generalizable to nonresponding physicians within Nova Scotia collaborating with NPs or to physicians not in collaborative practices with NPs as they were not included as a part of the sample. Due to multiple statistical comparisons (Mann Whitney U), the results comparing NP and physician ratings of results should be interpreted with caution.<br><br>
Conclusion
Respondent ratings of resources and preferences for resource use were consistent with self-reported means of conducting searches for specific drug information queries. The use of computers and PDAs remains limited and also matches preferences and resource ratings. Education to this group of practitioners regarding available drug information resources may facilitate use of computer and PDA resources. Further research is needed to determine methods to increase the use of computers and PDAs and if use of these technologies affects prescribing and <span class="yellow">patient</span> outcomes.<br><br>Competing interests
Ingrid Sketris holds a Chair from Canadian Institutes of Health Research (CIHR), Canadian Health Services Research Foundation (CHSRF) co-sponsored by the Nova Scotia Health Research Foundation (NSHRF). Andrea Murphy received salary support through this Chair as a research fellow at the time of conducting this research. The survey was performed in fulfillment of the requirements for the Drug Use Management and Policy Residency that Murphy participated in as a part of her fellowship. The residency was conducted with a decision making partner from the Nova Scotia Department of Health.
The opinions expressed in this paper are those of the authors and do not represent the opinions of the Nova Scotia Department of Health, CIHR/CHSRF or NSHRF.
MF, MM, RMM, and DG have no competing interests to declare.<br><br>Authors' contributions
AM conceptualized the design and composed the survey instruments, carried out the study, entered and analyzed the data, drafted the original manuscript, and modified subsequent drafts based on authors' and reviewers' feedback. MF, RMM, IS, MM, and DG reviewed and suggested revisions to the survey tools, covering letters, overall study design, and contributed to feedback on the analysis and manuscript revisions.<br><br>Pre-publication history
The pre-publication history for this paper can be accessed here:<br><br>
Supplementary Material<br><br>
</body></html>